# MetricResult


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

------------------------------------------------------------------------

<a
href="https://github.com/explodinggradients/ragas_experimental/blob/main/ragas_experimental/metric/result.py#L14"
target="_blank" style="float:right; font-size:smaller">source</a>

### MetricResult

>  MetricResult (result:Any, reason:Optional[str]=None,
>                    traces:Optional[Dict[str,Any]]=None)

\*Class to hold the result of a metric evaluation.

This class behaves like its underlying result value but still provides
access to additional metadata like reasoning.

Works with: - DiscreteMetrics (string results) - NumericMetrics
(float/int results) - RankingMetrics (list results)\*

### Example Usage

``` python
metric_result = MetricResult(result=42, reason="This is a test")
print(metric_result)
print(metric_result.reason)

### Example with Numeric Operations
num_result1 = MetricResult(result=5.0)
num_result2 = MetricResult(result=3.0)
print(num_result1 + num_result2)  # 8.0


### Example with String Operations
str_result = MetricResult(result="low")
print(str_result.upper())  # "LOW"

## Example with List Operations
list_result = MetricResult(result=[1, 2, 3])
print(list_result[1:])  # 2
```

    42
    This is a test
    8.0
    LOW
    [2, 3]

now lets make it `Pydantic` compatible also

------------------------------------------------------------------------

<a
href="https://github.com/explodinggradients/ragas_experimental/blob/main/ragas_experimental/metric/result.py#L214"
target="_blank" style="float:right; font-size:smaller">source</a>

### MetricResult.\_\_get_pydantic_core_schema\_\_

>  MetricResult.__get_pydantic_core_schema__ (_source_type:Any,
>                                                 _handler:pydantic.annotated_ha
>                                                 ndlers.GetCoreSchemaHandler)

\*Generate a Pydantic core schema for MetricResult.

This custom schema handles different serialization behaviors: - For
model_dump(): Returns the original MetricResult instance - For
model_dump_json(): Converts to a JSON-compatible dict using **json**\*

------------------------------------------------------------------------

<a
href="https://github.com/explodinggradients/ragas_experimental/blob/main/ragas_experimental/metric/result.py#L201"
target="_blank" style="float:right; font-size:smaller">source</a>

### MetricResult.\_\_json\_\_

>  MetricResult.__json__ ()

\*Return data for JSON serialization.

This method is used by json.dumps and other JSON serializers to convert
MetricResult to a JSON-compatible format.\*

------------------------------------------------------------------------

<a
href="https://github.com/explodinggradients/ragas_experimental/blob/main/ragas_experimental/metric/result.py#L193"
target="_blank" style="float:right; font-size:smaller">source</a>

### MetricResult.validate

>  MetricResult.validate (value:Any,
>                             info:pydantic_core.core_schema.ValidationInfo)

*Provide compatibility with older Pydantic versions.*

``` python
from pydantic import BaseModel

class TestModel(BaseModel):
    response: str
    grade: MetricResult
    faithfulness: MetricResult
```

``` python
m = TestModel(response="test", grade=MetricResult(result=1, reason="test"), faithfulness=MetricResult(result=1, reason="test"))
m
```

    TestModel(response='test', grade=1, faithfulness=1)

``` python
m.model_dump()
```

    {'response': 'test', 'grade': 1, 'faithfulness': 1}

``` python
m.model_dump_json()
```

    '{"response":"test","grade":{"result":1,"reason":"test"},"faithfulness":{"result":1,"reason":"test"}}'
