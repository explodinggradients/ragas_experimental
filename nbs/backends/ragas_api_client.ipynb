{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Ragas API Client`\n",
    "\n",
    "> Python client to api.ragas.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp backends.ragas_api_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAGAS_APP_TOKEN = \"apt.47bd-c55e4a45b27c-02f8-8446-1441f09b-651a8\"\n",
    "RAGAS_API_ENDPOINT = \"https://api.dev.app.ragas.io\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import httpx\n",
    "import asyncio\n",
    "import typing as t\n",
    "from pydantic import BaseModel, Field\n",
    "from enum import StrEnum\n",
    "import uuid\n",
    "from fastcore.utils import patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RagasRelay:\n",
    "    \"\"\"Client for the Ragas Relay API.\"\"\"\n",
    "\n",
    "    def __init__(self, base_url: str, app_token: t.Optional[str] = None):\n",
    "        \"\"\"Initialize the Ragas API client.\n",
    "        \n",
    "        Args:\n",
    "            base_url: Base URL for the API (e.g., \"http://localhost:8087\")\n",
    "            app_token: API token for authentication\n",
    "        \"\"\"\n",
    "        if not app_token:\n",
    "            raise ValueError(\"app_token must be provided\")\n",
    "\n",
    "        self.base_url = f\"{base_url.rstrip('/')}/api/v1\"\n",
    "        self.app_token = app_token\n",
    "\n",
    "    async def _request(\n",
    "        self,\n",
    "        method: str,\n",
    "        endpoint: str,\n",
    "        params: t.Optional[t.Dict] = None,\n",
    "        json_data: t.Optional[t.Dict] = None,\n",
    "    ) -> t.Dict:\n",
    "        \"\"\"Make a request to the API.\n",
    "        \n",
    "        Args:\n",
    "            method: HTTP method (GET, POST, PATCH, DELETE)\n",
    "            endpoint: API endpoint path\n",
    "            params: Query parameters\n",
    "            json_data: JSON request body\n",
    "            \n",
    "        Returns:\n",
    "            The response data from the API\n",
    "        \"\"\"\n",
    "        url = f\"{self.base_url}/{endpoint.lstrip('/')}\"\n",
    "        headers = {\"X-App-Token\": self.app_token}\n",
    "\n",
    "        async with httpx.AsyncClient() as client:\n",
    "            response = await client.request(\n",
    "                method=method, url=url, params=params, json=json_data, headers=headers\n",
    "            )\n",
    "\n",
    "            data = response.json()\n",
    "\n",
    "            if response.status_code >= 400 or data.get(\"status\") == \"error\":\n",
    "                error_msg = data.get(\"message\", \"Unknown error\")\n",
    "                raise Exception(f\"API Error ({response.status_code}): {error_msg}\")\n",
    "\n",
    "            return data.get(\"data\")\n",
    "\n",
    "    #---- Resource Handlers ----\n",
    "    async def _create_resource(self, path, data):\n",
    "        \"\"\"Generic resource creation.\"\"\"\n",
    "        return await self._request(\"POST\", path, json_data=data)\n",
    "        \n",
    "    async def _list_resources(self, path, **params):\n",
    "        \"\"\"Generic resource listing.\"\"\"\n",
    "        return await self._request(\"GET\", path, params=params)\n",
    "        \n",
    "    async def _get_resource(self, path):\n",
    "        \"\"\"Generic resource retrieval.\"\"\"\n",
    "        return await self._request(\"GET\", path)\n",
    "        \n",
    "    async def _update_resource(self, path, data):\n",
    "        \"\"\"Generic resource update.\"\"\"\n",
    "        return await self._request(\"PATCH\", path, json_data=data)\n",
    "        \n",
    "    async def _delete_resource(self, path):\n",
    "        \"\"\"Generic resource deletion.\"\"\"\n",
    "        return await self._request(\"DELETE\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#---- Projects ----\n",
    "@patch\n",
    "async def list_projects(\n",
    "    self: RagasRelay,\n",
    "    ids: t.Optional[t.List[str]] = None,\n",
    "    limit: int = 50,\n",
    "    offset: int = 0,\n",
    "    order_by: t.Optional[str] = None,\n",
    "    sort_dir: t.Optional[str] = None,\n",
    ") -> t.Dict:\n",
    "    \"\"\"List projects.\"\"\"\n",
    "    params = {\"limit\": limit, \"offset\": offset}\n",
    "\n",
    "    if ids:\n",
    "        params[\"ids\"] = \",\".join(ids)\n",
    "\n",
    "    if order_by:\n",
    "        params[\"order_by\"] = order_by\n",
    "\n",
    "    if sort_dir:\n",
    "        params[\"sort_dir\"] = sort_dir\n",
    "\n",
    "    return await self._list_resources(\"projects\", **params)\n",
    "\n",
    "@patch\n",
    "async def get_project(self: RagasRelay, project_id: str) -> t.Dict:\n",
    "    \"\"\"Get a specific project by ID.\"\"\"\n",
    "    return await self._get_resource(f\"projects/{project_id}\")\n",
    "\n",
    "@patch\n",
    "async def create_project(\n",
    "    self: RagasRelay, title: str, description: t.Optional[str] = None\n",
    ") -> t.Dict:\n",
    "    \"\"\"Create a new project.\"\"\"\n",
    "    data = {\"title\": title}\n",
    "    if description:\n",
    "        data[\"description\"] = description\n",
    "    return await self._create_resource(\"projects\", data)\n",
    "\n",
    "@patch\n",
    "async def update_project(\n",
    "    self: RagasRelay,\n",
    "    project_id: str,\n",
    "    title: t.Optional[str] = None,\n",
    "    description: t.Optional[str] = None,\n",
    ") -> t.Dict:\n",
    "    \"\"\"Update an existing project.\"\"\"\n",
    "    data = {}\n",
    "    if title:\n",
    "        data[\"title\"] = title\n",
    "    if description:\n",
    "        data[\"description\"] = description\n",
    "    return await self._update_resource(f\"projects/{project_id}\", data)\n",
    "\n",
    "@patch\n",
    "async def delete_project(self: RagasRelay, project_id: str) -> None:\n",
    "    \"\"\"Delete a project.\"\"\"\n",
    "    await self._delete_resource(f\"projects/{project_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 projects:\n",
      "Error: string indices must be integers, not 'str'\n"
     ]
    }
   ],
   "source": [
    "# Initialize client with your authentication token\n",
    "client = RagasRelay(base_url=RAGAS_API_ENDPOINT, app_token=RAGAS_APP_TOKEN)\n",
    "\n",
    "# List projects\n",
    "try:\n",
    "    projects = await client.list_projects(limit=10)\n",
    "    print(f\"Found {len(projects)} projects:\")\n",
    "    for project in projects:\n",
    "        print(f\"- {project['title']} (ID: {project['id']})\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '59cf2483-d2c7-4306-af87-bfac2813f27b',\n",
       " 'title': 'test project',\n",
       " 'description': 'test description',\n",
       " 'created_at': '2025-04-09T05:57:57.991728+00:00',\n",
       " 'updated_at': '2025-04-09T05:57:57.991728+00:00'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await client.create_project(\"test project\", \"test description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'items': [{'id': '59cf2483-d2c7-4306-af87-bfac2813f27b',\n",
       "   'title': 'test project',\n",
       "   'description': 'test description',\n",
       "   'created_at': '2025-04-09T05:57:57.991728+00:00',\n",
       "   'updated_at': '2025-04-09T05:57:57.991728+00:00'},\n",
       "  {'id': 'c026b63c-d618-42c0-81c3-d7824c976eb1',\n",
       "   'title': 'test project',\n",
       "   'description': 'test description',\n",
       "   'created_at': '2025-04-08T22:46:04.045516+00:00',\n",
       "   'updated_at': '2025-04-08T22:46:04.045516+00:00'},\n",
       "  {'id': '3dd738de-49f7-494c-aa0a-f6531d3b603a',\n",
       "   'title': 'RagasTest',\n",
       "   'description': '',\n",
       "   'created_at': '2025-04-08T17:45:32.759553+00:00',\n",
       "   'updated_at': '2025-04-08T17:45:32.759553+00:00'},\n",
       "  {'id': '2f45d026-1b13-4851-a36d-c7680edb6380',\n",
       "   'title': 'test project',\n",
       "   'description': 'test description',\n",
       "   'created_at': '2025-04-08T14:38:14.61165+00:00',\n",
       "   'updated_at': '2025-04-08T14:38:14.61165+00:00'},\n",
       "  {'id': 'e1b3f1e4-d344-48f4-a178-84e7e32e6ab6',\n",
       "   'title': 'test project',\n",
       "   'description': 'test description',\n",
       "   'created_at': '2025-03-30T02:33:38.751793+00:00',\n",
       "   'updated_at': '2025-03-30T02:33:38.751793+00:00'}],\n",
       " 'pagination': {'offset': 0,\n",
       "  'limit': 50,\n",
       "  'total': 5,\n",
       "  'order_by': 'created_at',\n",
       "  'sort_dir': 'desc'}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await client.list_projects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_PROJECT_ID = \"e1b3f1e4-d344-48f4-a178-84e7e32e6ab6\"\n",
    "project = await client.get_project(TEST_PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "#---- Datasets ----\n",
    "@patch\n",
    "async def list_datasets(\n",
    "    self: RagasRelay,\n",
    "    project_id: str,\n",
    "    limit: int = 50,\n",
    "    offset: int = 0,\n",
    "    order_by: t.Optional[str] = None,\n",
    "    sort_dir: t.Optional[str] = None,\n",
    ") -> t.Dict:\n",
    "    \"\"\"List datasets in a project.\"\"\"\n",
    "    params = {\"limit\": limit, \"offset\": offset}\n",
    "    if order_by:\n",
    "        params[\"order_by\"] = order_by\n",
    "    if sort_dir:\n",
    "        params[\"sort_dir\"] = sort_dir\n",
    "    return await self._list_resources(f\"projects/{project_id}/datasets\", **params)\n",
    "\n",
    "@patch\n",
    "async def get_dataset(self: RagasRelay, project_id: str, dataset_id: str) -> t.Dict:\n",
    "    \"\"\"Get a specific dataset.\"\"\"\n",
    "    return await self._get_resource(f\"projects/{project_id}/datasets/{dataset_id}\")\n",
    "\n",
    "@patch\n",
    "async def create_dataset(\n",
    "    self: RagasRelay, project_id: str, name: str, description: t.Optional[str] = None\n",
    ") -> t.Dict:\n",
    "    \"\"\"Create a new dataset in a project.\"\"\"\n",
    "    data = {\"name\": name}\n",
    "    if description:\n",
    "        data[\"description\"] = description\n",
    "    return await self._create_resource(f\"projects/{project_id}/datasets\", data)\n",
    "\n",
    "@patch\n",
    "async def update_dataset(\n",
    "    self: RagasRelay,\n",
    "    project_id: str,\n",
    "    dataset_id: str,\n",
    "    name: t.Optional[str] = None,\n",
    "    description: t.Optional[str] = None,\n",
    ") -> t.Dict:\n",
    "    \"\"\"Update an existing dataset.\"\"\"\n",
    "    data = {}\n",
    "    if name:\n",
    "        data[\"name\"] = name\n",
    "    if description:\n",
    "        data[\"description\"] = description\n",
    "    return await self._update_resource(f\"projects/{project_id}/datasets/{dataset_id}\", data)\n",
    "\n",
    "@patch\n",
    "async def delete_dataset(self: RagasRelay, project_id: str, dataset_id: str) -> None:\n",
    "    \"\"\"Delete a dataset.\"\"\"\n",
    "    await self._delete_resource(f\"projects/{project_id}/datasets/{dataset_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('59cf2483-d2c7-4306-af87-bfac2813f27b',\n",
       " 'e1b3f1e4-d344-48f4-a178-84e7e32e6ab6')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check project ID\n",
    "projects = await client.list_projects()\n",
    "projects[\"items\"][0][\"id\"], TEST_PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset created: {'id': '656e632c-6f0b-4046-929e-05b98fb14eaa', 'name': 'New Dataset', 'description': 'This is a new dataset', 'updated_at': '2025-04-09T05:59:04.352084+00:00', 'created_at': '2025-04-09T05:59:04.352084+00:00', 'version_counter': 0, 'project_id': '59cf2483-d2c7-4306-af87-bfac2813f27b'}\n"
     ]
    }
   ],
   "source": [
    "# Create a new dataset\n",
    "new_dataset = await client.create_dataset(\n",
    "    projects[\"items\"][0][\"id\"], \"New Dataset\", \"This is a new dataset\"\n",
    ")\n",
    "print(f\"New dataset created: {new_dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 datasets\n"
     ]
    }
   ],
   "source": [
    "# List datasets in the project\n",
    "datasets = await client.list_datasets(projects[\"items\"][0][\"id\"])\n",
    "print(f\"Found {len(datasets)} datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated dataset: {'id': '656e632c-6f0b-4046-929e-05b98fb14eaa', 'name': 'Updated Dataset', 'description': 'This is an updated dataset', 'created_at': '2025-04-09T05:59:04.352084+00:00', 'updated_at': '2025-04-09T05:59:10.388986+00:00', 'version_counter': 0, 'project_id': '59cf2483-d2c7-4306-af87-bfac2813f27b'}\n"
     ]
    }
   ],
   "source": [
    "updated_dataset = await client.update_dataset(\n",
    "    projects[\"items\"][0][\"id\"],\n",
    "    datasets[\"items\"][0][\"id\"],\n",
    "    \"Updated Dataset\",\n",
    "    \"This is an updated dataset\",\n",
    ")\n",
    "print(f\"Updated dataset: {updated_dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset deleted\n"
     ]
    }
   ],
   "source": [
    "# Delete the dataset\n",
    "await client.delete_dataset(projects[\"items\"][0][\"id\"], datasets[\"items\"][0][\"id\"])\n",
    "print(\"Dataset deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    " #| export\n",
    "#---- Experiments ----\n",
    "@patch\n",
    "async def list_experiments(\n",
    "    self: RagasRelay,\n",
    "    project_id: str,\n",
    "    limit: int = 50,\n",
    "    offset: int = 0,\n",
    "    order_by: t.Optional[str] = None,\n",
    "    sort_dir: t.Optional[str] = None,\n",
    ") -> t.Dict:\n",
    "    \"\"\"List experiments in a project.\"\"\"\n",
    "    params = {\"limit\": limit, \"offset\": offset}\n",
    "    if order_by:\n",
    "        params[\"order_by\"] = order_by\n",
    "    if sort_dir:\n",
    "        params[\"sort_dir\"] = sort_dir\n",
    "    return await self._list_resources(f\"projects/{project_id}/experiments\", **params)\n",
    "\n",
    "@patch\n",
    "async def get_experiment(self: RagasRelay, project_id: str, experiment_id: str) -> t.Dict:\n",
    "    \"\"\"Get a specific experiment.\"\"\"\n",
    "    return await self._get_resource(f\"projects/{project_id}/experiments/{experiment_id}\")\n",
    "\n",
    "@patch\n",
    "async def create_experiment(\n",
    "    self: RagasRelay, project_id: str, name: str, description: t.Optional[str] = None\n",
    ") -> t.Dict:\n",
    "    \"\"\"Create a new experiment in a project.\"\"\"\n",
    "    data = {\"name\": name}\n",
    "    if description:\n",
    "        data[\"description\"] = description\n",
    "    return await self._create_resource(f\"projects/{project_id}/experiments\", data)\n",
    "\n",
    "@patch\n",
    "async def update_experiment(\n",
    "    self: RagasRelay,\n",
    "    project_id: str,\n",
    "    experiment_id: str,\n",
    "    name: t.Optional[str] = None,\n",
    "    description: t.Optional[str] = None,\n",
    ") -> t.Dict:\n",
    "    \"\"\"Update an existing experiment.\"\"\"\n",
    "    data = {}\n",
    "    if name:\n",
    "        data[\"name\"] = name\n",
    "    if description:\n",
    "        data[\"description\"] = description\n",
    "    return await self._update_resource(f\"projects/{project_id}/experiments/{experiment_id}\", data)\n",
    "\n",
    "@patch\n",
    "async def delete_experiment(self: RagasRelay, project_id: str, experiment_id: str) -> None:\n",
    "    \"\"\"Delete an experiment.\"\"\"\n",
    "    await self._delete_resource(f\"projects/{project_id}/experiments/{experiment_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New experiment created: {'id': '783c34ec-9d92-4bf9-9a45-d64746e58ba9', 'name': 'New Experiment', 'description': 'This is a new experiment', 'updated_at': '2025-04-09T06:00:01.80579+00:00', 'created_at': '2025-04-09T06:00:01.80579+00:00', 'version_counter': 0, 'project_id': '59cf2483-d2c7-4306-af87-bfac2813f27b'}\n",
      "Found 2 experiments\n",
      "Experiment: {'id': '783c34ec-9d92-4bf9-9a45-d64746e58ba9', 'name': 'New Experiment', 'description': 'This is a new experiment', 'created_at': '2025-04-09T06:00:01.80579+00:00', 'updated_at': '2025-04-09T06:00:01.80579+00:00', 'version_counter': 0, 'project_id': '59cf2483-d2c7-4306-af87-bfac2813f27b'}\n",
      "Updated experiment: {'id': '783c34ec-9d92-4bf9-9a45-d64746e58ba9', 'name': 'Updated Experiment', 'description': 'This is an updated experiment', 'created_at': '2025-04-09T06:00:01.80579+00:00', 'updated_at': '2025-04-09T06:00:04.394175+00:00', 'version_counter': 0, 'project_id': '59cf2483-d2c7-4306-af87-bfac2813f27b'}\n",
      "Experiment deleted\n"
     ]
    }
   ],
   "source": [
    "# create a new experiment\n",
    "new_experiment = await client.create_experiment(\n",
    "    projects[\"items\"][0][\"id\"], \"New Experiment\", \"This is a new experiment\"\n",
    ")\n",
    "print(f\"New experiment created: {new_experiment}\")\n",
    "# list experiments\n",
    "experiments = await client.list_experiments(projects[\"items\"][0][\"id\"])\n",
    "print(f\"Found {len(experiments)} experiments\")\n",
    "# get a specific experiment\n",
    "experiment = await client.get_experiment(\n",
    "    projects[\"items\"][0][\"id\"], experiments[\"items\"][0][\"id\"]\n",
    ")\n",
    "print(f\"Experiment: {experiment}\")\n",
    "# update an experiment\n",
    "updated_experiment = await client.update_experiment(\n",
    "    projects[\"items\"][0][\"id\"],\n",
    "    experiments[\"items\"][0][\"id\"],\n",
    "    \"Updated Experiment\",\n",
    "    \"This is an updated experiment\",\n",
    ")\n",
    "print(f\"Updated experiment: {updated_experiment}\")\n",
    "# delete an experiment\n",
    "await client.delete_experiment(projects[\"items\"][0][\"id\"], experiments[\"items\"][0][\"id\"])\n",
    "print(\"Experiment deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'items': [{'id': '78fd6c58-7edf-4239-93d1-4f49185d8e49',\n",
       "   'name': 'New Experiment',\n",
       "   'description': 'This is a new experiment',\n",
       "   'created_at': '2025-03-30T06:31:31.689269+00:00',\n",
       "   'updated_at': '2025-03-30T06:31:31.689269+00:00',\n",
       "   'project_id': 'e1b3f1e4-d344-48f4-a178-84e7e32e6ab6'}],\n",
       " 'pagination': {'offset': 0,\n",
       "  'limit': 50,\n",
       "  'total': 1,\n",
       "  'order_by': 'created_at',\n",
       "  'sort_dir': 'asc'}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await client.list_experiments(TEST_PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns (for datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API supports the following column types:\n",
    "\n",
    "- `number`: Numeric values\n",
    "- `longText`: Text content\n",
    "- `select`: Single selection from predefined options\n",
    "- `date`: Date values\n",
    "- `multiSelect`: Multiple selections from predefined options\n",
    "- `checkbox`: Boolean values\n",
    "- `custom`: Custom column types with specific behavior\n",
    "\n",
    "Each column type has specific settings that can be configured through the `settings` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ColumnType(StrEnum):\n",
    "    NUMBER = \"number\"\n",
    "    TEXT = \"text\"\n",
    "    LONG_TEXT = \"longText\"\n",
    "    SELECT = \"select\"\n",
    "    DATE = \"date\"\n",
    "    MULTI_SELECT = \"multiSelect\"\n",
    "    CHECKBOX = \"checkbox\"\n",
    "    CUSTOM = \"custom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "#---- Dataset Columns ----\n",
    "@patch\n",
    "async def list_dataset_columns(\n",
    "    self: RagasRelay,\n",
    "    project_id: str,\n",
    "    dataset_id: str,\n",
    "    limit: int = 50,\n",
    "    offset: int = 0,\n",
    "    order_by: t.Optional[str] = None,\n",
    "    sort_dir: t.Optional[str] = None,\n",
    ") -> t.Dict:\n",
    "    \"\"\"List columns in a dataset.\"\"\"\n",
    "    params = {\"limit\": limit, \"offset\": offset}\n",
    "    if order_by:\n",
    "        params[\"order_by\"] = order_by\n",
    "    if sort_dir:\n",
    "        params[\"sort_dir\"] = sort_dir\n",
    "    return await self._list_resources(\n",
    "        f\"projects/{project_id}/datasets/{dataset_id}/columns\", **params\n",
    "    )\n",
    "\n",
    "@patch\n",
    "async def get_dataset_column(\n",
    "    self: RagasRelay, project_id: str, dataset_id: str, column_id: str\n",
    ") -> t.Dict:\n",
    "    \"\"\"Get a specific column in a dataset.\"\"\"\n",
    "    return await self._get_resource(\n",
    "        f\"projects/{project_id}/datasets/{dataset_id}/columns/{column_id}\"\n",
    "    )\n",
    "\n",
    "@patch\n",
    "async def create_dataset_column(\n",
    "    self: RagasRelay,\n",
    "    project_id: str,\n",
    "    dataset_id: str,\n",
    "    id: str,\n",
    "    name: str,\n",
    "    type: str,\n",
    "    col_order: t.Optional[int] = None,\n",
    "    settings: t.Optional[t.Dict] = None,\n",
    ") -> t.Dict:\n",
    "    \"\"\"Create a new column in a dataset.\"\"\"\n",
    "    data = {\"id\": id, \"name\": name, \"type\": type}\n",
    "    if col_order is not None:\n",
    "        data[\"col_order\"] = col_order\n",
    "    if settings:\n",
    "        data[\"settings\"] = settings\n",
    "    return await self._create_resource(\n",
    "        f\"projects/{project_id}/datasets/{dataset_id}/columns\", data\n",
    "    )\n",
    "\n",
    "@patch\n",
    "async def update_dataset_column(\n",
    "    self: RagasRelay, project_id: str, dataset_id: str, column_id: str, **column_data\n",
    ") -> t.Dict:\n",
    "    \"\"\"Update an existing column in a dataset.\"\"\"\n",
    "    return await self._update_resource(\n",
    "        f\"projects/{project_id}/datasets/{dataset_id}/columns/{column_id}\",\n",
    "        column_data,\n",
    "    )\n",
    "\n",
    "@patch\n",
    "async def delete_dataset_column(\n",
    "    self: RagasRelay, project_id: str, dataset_id: str, column_id: str\n",
    ") -> None:\n",
    "    \"\"\"Delete a column from a dataset.\"\"\"\n",
    "    await self._delete_resource(\n",
    "        f\"projects/{project_id}/datasets/{dataset_id}/columns/{column_id}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '9f0f1ac6-beb6-4bda-95d4-9d2a7dc4b837',\n",
       " 'name': 'New Dataset for testing columns',\n",
       " 'description': 'This is a new dataset for testing columns',\n",
       " 'updated_at': '2025-04-09T06:01:40.671173+00:00',\n",
       " 'created_at': '2025-04-09T06:01:40.671173+00:00',\n",
       " 'version_counter': 0,\n",
       " 'project_id': '59cf2483-d2c7-4306-af87-bfac2813f27b'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = await client.create_dataset(\n",
    "    projects[\"items\"][0][\"id\"],\n",
    "    \"New Dataset for testing columns\",\n",
    "    \"This is a new dataset for testing columns\",\n",
    ")\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'new_column_3',\n",
       " 'name': 'New Column 3',\n",
       " 'type': 'longText',\n",
       " 'settings': {'id': 'new_column_3',\n",
       "  'name': 'New Column 3',\n",
       "  'type': 'longText',\n",
       "  'max_length': 255,\n",
       "  'is_required': True},\n",
       " 'created_at': '2025-04-09T06:01:41.978634+00:00',\n",
       " 'updated_at': '2025-04-09T06:01:41.978634+00:00',\n",
       " 'datatable_id': '9f0f1ac6-beb6-4bda-95d4-9d2a7dc4b837'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add a new column to the dataset\n",
    "new_column = await client.create_dataset_column(\n",
    "    project_id=projects[\"items\"][0][\"id\"],\n",
    "    dataset_id=datasets[\"id\"],\n",
    "    id=\"new_column_3\",\n",
    "    name=\"New Column 3\",\n",
    "    type=ColumnType.TEXT.value,\n",
    "    settings={\n",
    "        \"max_length\": 255,\n",
    "        \"is_required\": True,\n",
    "    },\n",
    ")\n",
    "new_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'items': [{'id': 'new_column_3',\n",
       "   'name': 'New Column 3',\n",
       "   'type': 'longText',\n",
       "   'settings': {'id': 'new_column_3',\n",
       "    'name': 'New Column 3',\n",
       "    'type': 'longText',\n",
       "    'max_length': 255,\n",
       "    'is_required': True},\n",
       "   'created_at': '2025-04-09T06:01:41.978634+00:00',\n",
       "   'updated_at': '2025-04-09T06:01:41.978634+00:00',\n",
       "   'datatable_id': '9f0f1ac6-beb6-4bda-95d4-9d2a7dc4b837'}],\n",
       " 'pagination': {'offset': 0,\n",
       "  'limit': 50,\n",
       "  'total': 1,\n",
       "  'order_by': 'created_at',\n",
       "  'sort_dir': 'asc'}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await client.list_dataset_columns(projects[\"items\"][0][\"id\"], datasets[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'new_column_3',\n",
       " 'name': 'New Column 3',\n",
       " 'type': 'longText',\n",
       " 'settings': {'id': 'new_column_3',\n",
       "  'name': 'New Column 3',\n",
       "  'type': 'longText',\n",
       "  'max_length': 255,\n",
       "  'is_required': True},\n",
       " 'created_at': '2025-04-09T06:01:41.978634+00:00',\n",
       " 'updated_at': '2025-04-09T06:01:41.978634+00:00',\n",
       " 'datatable_id': '9f0f1ac6-beb6-4bda-95d4-9d2a7dc4b837'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col3 = await client.get_dataset_column(\n",
    "    projects[\"items\"][0][\"id\"], datasets[\"id\"], \"new_column_3\"\n",
    ")\n",
    "col3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'new_column_3',\n",
       " 'name': 'New Column 3 Updated',\n",
       " 'type': 'number',\n",
       " 'settings': {'id': 'new_column_3',\n",
       "  'name': 'New Column 3',\n",
       "  'type': 'longText',\n",
       "  'max_length': 255,\n",
       "  'is_required': True},\n",
       " 'created_at': '2025-04-09T06:01:41.978634+00:00',\n",
       " 'updated_at': '2025-04-09T06:01:46.946099+00:00',\n",
       " 'datatable_id': '9f0f1ac6-beb6-4bda-95d4-9d2a7dc4b837'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await client.update_dataset_column(\n",
    "    projects[\"items\"][0][\"id\"],\n",
    "    datasets[\"id\"],\n",
    "    \"new_column_3\",\n",
    "    name=\"New Column 3 Updated\",\n",
    "    type=ColumnType.NUMBER.value,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "await client.delete_dataset_column(\n",
    "    projects[\"items\"][0][\"id\"], datasets[\"id\"], \"new_column_3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rows (for datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---- Dataset Rows ----\n",
    "@patch\n",
    "async def list_dataset_rows(\n",
    "    self: RagasRelay,\n",
    "    project_id: str,\n",
    "    dataset_id: str,\n",
    "    limit: int = 50,\n",
    "    offset: int = 0,\n",
    "    order_by: t.Optional[str] = None,\n",
    "    sort_dir: t.Optional[str] = None,\n",
    ") -> t.Dict:\n",
    "    \"\"\"List rows in a dataset.\"\"\"\n",
    "    params = {\"limit\": limit, \"offset\": offset}\n",
    "    if order_by:\n",
    "        params[\"order_by\"] = order_by\n",
    "    if sort_dir:\n",
    "        params[\"sort_dir\"] = sort_dir\n",
    "    return await self._list_resources(\n",
    "        f\"projects/{project_id}/datasets/{dataset_id}/rows\", **params\n",
    "    )\n",
    "\n",
    "@patch\n",
    "async def get_dataset_row(\n",
    "    self: RagasRelay, project_id: str, dataset_id: str, row_id: str\n",
    ") -> t.Dict:\n",
    "    \"\"\"Get a specific row in a dataset.\"\"\"\n",
    "    return await self._get_resource(\n",
    "        f\"projects/{project_id}/datasets/{dataset_id}/rows/{row_id}\"\n",
    "    )\n",
    "\n",
    "@patch\n",
    "async def create_dataset_row(\n",
    "    self: RagasRelay, project_id: str, dataset_id: str, id: str, data: t.Dict\n",
    ") -> t.Dict:\n",
    "    \"\"\"Create a new row in a dataset.\"\"\"\n",
    "    row_data = {\"id\": id, \"data\": data}\n",
    "    return await self._create_resource(\n",
    "        f\"projects/{project_id}/datasets/{dataset_id}/rows\", row_data\n",
    "    )\n",
    "\n",
    "@patch\n",
    "async def update_dataset_row(\n",
    "    self: RagasRelay, project_id: str, dataset_id: str, row_id: str, data: t.Dict\n",
    ") -> t.Dict:\n",
    "    \"\"\"Update an existing row in a dataset.\"\"\"\n",
    "    row_data = {\"data\": data}\n",
    "    return await self._update_resource(\n",
    "        f\"projects/{project_id}/datasets/{dataset_id}/rows/{row_id}\",\n",
    "        row_data,\n",
    "    )\n",
    "\n",
    "@patch\n",
    "async def delete_dataset_row(\n",
    "    self: RagasRelay, project_id: str, dataset_id: str, row_id: str\n",
    ") -> None:\n",
    "    \"\"\"Delete a row from a dataset.\"\"\"\n",
    "    await self._delete_resource(\n",
    "        f\"projects/{project_id}/datasets/{dataset_id}/rows/{row_id}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "API Error (500): Failed to create row",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m client.create_dataset_row(\n\u001b[32m      2\u001b[39m     projects[\u001b[33m\"\u001b[39m\u001b[33mitems\u001b[39m\u001b[33m\"\u001b[39m][\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      3\u001b[39m     datasets[\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mNew Row 1\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mage\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m30\u001b[39m},\n\u001b[32m      6\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mcreate_dataset_row\u001b[39m\u001b[34m(self, project_id, dataset_id, id, data)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Create a new row in a dataset.\"\"\"\u001b[39;00m\n\u001b[32m     36\u001b[39m row_data = {\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mid\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m: data}\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_resource(\n\u001b[32m     38\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mprojects/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproject_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/datasets/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/rows\u001b[39m\u001b[33m\"\u001b[39m, row_data\n\u001b[32m     39\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mRagasRelay._create_resource\u001b[39m\u001b[34m(self, path, data)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_resource\u001b[39m(\u001b[38;5;28mself\u001b[39m, path, data):\n\u001b[32m     54\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Generic resource creation.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request(\u001b[33m\"\u001b[39m\u001b[33mPOST\u001b[39m\u001b[33m\"\u001b[39m, path, json_data=data)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mRagasRelay._request\u001b[39m\u001b[34m(self, method, endpoint, params, json_data)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code >= \u001b[32m400\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m data.get(\u001b[33m\"\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     47\u001b[39m     error_msg = data.get(\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mUnknown error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAPI Error (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data.get(\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mException\u001b[39m: API Error (500): Failed to create row"
     ]
    }
   ],
   "source": [
    "await client.create_dataset_row(\n",
    "    projects[\"items\"][0][\"id\"],\n",
    "    datasets[\"id\"],\n",
    "    \"1\",\n",
    "    {\"name\": \"New Row 1\", \"age\": 30},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'items': [],\n",
       " 'pagination': {'offset': 0,\n",
       "  'limit': 50,\n",
       "  'total': 0,\n",
       "  'order_by': 'created_at',\n",
       "  'sort_dir': 'asc'}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await client.list_dataset_rows(projects[\"items\"][0][\"id\"], datasets[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import uuid\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def create_nano_id(size=12):\n",
    "    # Define characters to use (alphanumeric)\n",
    "    alphabet = string.ascii_letters + string.digits\n",
    "    \n",
    "    # Generate UUID and convert to int\n",
    "    uuid_int = uuid.uuid4().int\n",
    "    \n",
    "    # Convert to base62\n",
    "    result = \"\"\n",
    "    while uuid_int:\n",
    "        uuid_int, remainder = divmod(uuid_int, len(alphabet))\n",
    "        result = alphabet[remainder] + result\n",
    "    \n",
    "    # Pad if necessary and return desired length\n",
    "    return result[:size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eAscOAbBMSv2'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usage\n",
    "nano_id = create_nano_id()  # e.g., \"8dK9cNw3mP5x\"\n",
    "nano_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a Dataset Visualized - Created From UI\n",
    "Lets Create a new dataset and add columns and rows via the endpoint to see how it behaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://dev.app.ragas.io/dashboard/projects/e1b3f1e4-d344-48f4-a178-84e7e32e6ab6/datasets/cc239f73-6546-4d5c-84e1-8c2ed8072edc'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate a dataset\n",
    "dataset = await client.create_dataset(\n",
    "    project_id=TEST_PROJECT_ID,\n",
    "    name=\"Dataset Visualized from UI\",\n",
    "    description=\"This is a dataset created from the UI\",\n",
    ")\n",
    "\n",
    "# show url\n",
    "WEB_ENDPOINT = \"https://dev.app.ragas.io\"\n",
    "url = f\"{WEB_ENDPOINT}/dashboard/projects/{TEST_PROJECT_ID}/datasets/{dataset['id']}\"\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list columns\n",
    "columns = await client.list_dataset_columns(TEST_PROJECT_ID, dataset[\"id\"])\n",
    "# list rows\n",
    "rows = await client.list_dataset_rows(TEST_PROJECT_ID, dataset[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'items': [],\n",
       " 'pagination': {'offset': 0,\n",
       "  'limit': 50,\n",
       "  'total': 0,\n",
       "  'order_by': 'created_at',\n",
       "  'sort_dir': 'asc'}}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'items': [],\n",
       " 'pagination': {'offset': 0,\n",
       "  'limit': 50,\n",
       "  'total': 0,\n",
       "  'order_by': 'created_at',\n",
       "  'sort_dir': 'asc'}}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Dataset from data\n",
    "\n",
    "we want to be able to use the API with python data like this `t.List[t.Dict]`.\n",
    "```py\n",
    "# how we want the data to look\n",
    "data = [\n",
    "    {\n",
    "        \"id\": \"1\",\n",
    "        \"query\": \"What is the capital of France?\",\n",
    "        \"persona\": \"John\",\n",
    "        \"ground_truth\": \"Paris\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"2\",\n",
    "        \"query\": \"What is the capital of Germany?\",\n",
    "        \"persona\": \"Jane\",\n",
    "        \"ground_truth\": \"Berlin\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"3\",\n",
    "        \"query\": \"What is the capital of Italy?\",\n",
    "        \"persona\": \"John\",\n",
    "        \"ground_truth\": \"Rome\",\n",
    "    },\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['number', 'text', 'longText', 'select', 'date', 'multiSelect', 'checkbox', 'custom']\n"
     ]
    }
   ],
   "source": [
    "# print out column types\n",
    "print([col.value for col in ColumnType])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it should be able to handle simple python dicts\n",
    "data = [\n",
    "    {\n",
    "        \"id\": \"1\",\n",
    "        \"query\": \"What is the capital of France?\",\n",
    "        \"persona\": \"John\",\n",
    "        \"ground_truth\": \"Paris\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"2\",\n",
    "        \"query\": \"What is the capital of Germany?\",\n",
    "        \"persona\": \"Jane\",\n",
    "        \"ground_truth\": \"Berlin\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There can be 2 ways to pass in data\n",
    "\n",
    "1. Data can come as either as simple dicts\n",
    "\n",
    "```py\n",
    "data = [\n",
    "    {\"column_1\": \"value\", \"column_2\": \"value\"}\n",
    "]\n",
    "```\n",
    "\n",
    "2. or if you want to give more settings\n",
    "\n",
    "```py\n",
    "data = [\n",
    "    {\n",
    "        \"column_1\": {\"data\": \"value\", \"type\": ColumnType.text},\n",
    "        \"column_2\": {\"data\": \"value\", \"type\": ColumnType.number},\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "3. after that you will have to pass a list `Column` and `Row` to add it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "test_data_columns = [\n",
    "    {\"name\": \"id\", \"type\": ColumnType.NUMBER.value},\n",
    "    {\"name\": \"query\", \"type\": ColumnType.TEXT.value},\n",
    "    {\"name\": \"persona\", \"type\": ColumnType.TEXT.value},\n",
    "    {\"name\": \"ground_truth\", \"type\": ColumnType.TEXT.value},\n",
    "]\n",
    "\n",
    "test_data_rows = [{\n",
    "    \"id\": \"1\",\n",
    "    \"query\": \"What is the capital of France?\",\n",
    "    \"persona\": \"John\",\n",
    "    \"ground_truth\": \"Paris\",\n",
    "}, {\n",
    "    \"id\": \"2\",\n",
    "    \"query\": \"What is the capital of Germany?\",\n",
    "    \"persona\": \"Jane\",\n",
    "    \"ground_truth\": \"Berlin\",\n",
    "}, {\n",
    "    \"id\": \"3\",\n",
    "    \"query\": \"What is the capital of Italy?\",\n",
    "    \"persona\": \"John\",\n",
    "    \"ground_truth\": \"Rome\",\n",
    "}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Default settings for columns\n",
    "DEFAULT_SETTINGS = {\n",
    "    \"is_required\": False,\n",
    "    \"max_length\": 1000\n",
    "}\n",
    "\n",
    "# Model definitions\n",
    "class Column(BaseModel):\n",
    "    id: str = Field(default_factory=create_nano_id)\n",
    "    name: str = Field(...)\n",
    "    type: str = Field(...)\n",
    "    settings: t.Dict = Field(default_factory=lambda: DEFAULT_SETTINGS.copy())\n",
    "    col_order: t.Optional[int] = Field(default=None)\n",
    "\n",
    "class RowCell(BaseModel):\n",
    "    data: t.Any = Field(...)\n",
    "    column_id: str = Field(...)\n",
    "\n",
    "class Row(BaseModel):\n",
    "    id: str = Field(default_factory=create_nano_id)\n",
    "    data: t.List[RowCell] = Field(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#---- Resource With Data Helper Methods ----\n",
    "@patch\n",
    "async def _create_with_data(\n",
    "    self: RagasRelay,\n",
    "    resource_type: str,\n",
    "    project_id: str,\n",
    "    name: str, \n",
    "    description: str,\n",
    "    columns: t.List[Column],\n",
    "    rows: t.List[Row],\n",
    "    batch_size: int = 50\n",
    ") -> t.Dict:\n",
    "    \"\"\"Generic method to create a resource with columns and rows.\n",
    "    \n",
    "    Args:\n",
    "        resource_type: Type of resource (\"dataset\" or \"experiment\")\n",
    "        project_id: Project ID\n",
    "        name: Resource name\n",
    "        description: Resource description\n",
    "        columns: List of column definitions\n",
    "        rows: List of row data\n",
    "        batch_size: Number of operations to perform concurrently\n",
    "        \n",
    "    Returns:\n",
    "        The created resource\n",
    "    \"\"\"\n",
    "    # Select appropriate methods based on resource type\n",
    "    if resource_type == \"dataset\":\n",
    "        create_fn = self.create_dataset\n",
    "        create_col_fn = self.create_dataset_column\n",
    "        create_row_fn = self.create_dataset_row\n",
    "        delete_fn = self.delete_dataset\n",
    "        id_key = \"dataset_id\"\n",
    "    elif resource_type == \"experiment\":\n",
    "        create_fn = self.create_experiment\n",
    "        create_col_fn = self.create_experiment_column\n",
    "        create_row_fn = self.create_experiment_row\n",
    "        delete_fn = self.delete_experiment\n",
    "        id_key = \"experiment_id\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported resource type: {resource_type}\")\n",
    "        \n",
    "    try:\n",
    "        # Create the resource\n",
    "        resource = await create_fn(project_id, name, description)\n",
    "        \n",
    "        # Process columns in batches\n",
    "        for i in range(0, len(columns), batch_size):\n",
    "            batch = columns[i:i+batch_size]\n",
    "            col_tasks = []\n",
    "            \n",
    "            for col in batch:\n",
    "                params = {\n",
    "                    \"project_id\": project_id,\n",
    "                    id_key: resource[\"id\"], # dataset_id here\n",
    "                    \"id\": col.id,\n",
    "                    \"name\": col.name,\n",
    "                    \"type\": col.type,\n",
    "                    \"settings\": col.settings\n",
    "                }\n",
    "                if col.col_order is not None:\n",
    "                    params[\"col_order\"] = col.col_order\n",
    "                \n",
    "                col_tasks.append(create_col_fn(**params))\n",
    "            \n",
    "            await asyncio.gather(*col_tasks)\n",
    "            \n",
    "        # Process rows in batches\n",
    "        for i in range(0, len(rows), batch_size):\n",
    "            batch = rows[i:i+batch_size]\n",
    "            row_tasks = []\n",
    "            \n",
    "            for row in batch:\n",
    "                row_data = {cell.column_id: cell.data for cell in row.data}\n",
    "                row_tasks.append(\n",
    "                    create_row_fn(\n",
    "                        project_id=project_id,\n",
    "                        **{id_key: resource[\"id\"]},\n",
    "                        id=row.id,\n",
    "                        data=row_data\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "            await asyncio.gather(*row_tasks)\n",
    "            \n",
    "        return resource\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Clean up on error\n",
    "        if 'resource' in locals():\n",
    "            try:\n",
    "                await delete_fn(project_id, resource[\"id\"])\n",
    "            except:\n",
    "                pass  # Ignore cleanup errors\n",
    "        raise e\n",
    "\n",
    "@patch\n",
    "async def create_dataset_with_data(\n",
    "    self: RagasRelay,\n",
    "    project_id: str,\n",
    "    name: str,\n",
    "    description: str,\n",
    "    columns: t.List[Column],\n",
    "    rows: t.List[Row],\n",
    "    batch_size: int = 50\n",
    ") -> t.Dict:\n",
    "    \"\"\"Create a dataset with columns and rows.\n",
    "    \n",
    "    This method creates a dataset and populates it with columns and rows in an\n",
    "    optimized way using concurrent requests.\n",
    "    \n",
    "    Args:\n",
    "        project_id: Project ID\n",
    "        name: Dataset name\n",
    "        description: Dataset description\n",
    "        columns: List of column definitions\n",
    "        rows: List of row data\n",
    "        batch_size: Number of operations to perform concurrently\n",
    "        \n",
    "    Returns:\n",
    "        The created dataset\n",
    "    \"\"\"\n",
    "    return await self._create_with_data(\n",
    "        \"dataset\", project_id, name, description, columns, rows, batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets test this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset with ID: 5e7912f4-6a65-4d0c-bf79-0fab9ddda40c\n",
      "Created 4 columns\n",
      "Created 3 rows\n"
     ]
    }
   ],
   "source": [
    "# First convert the raw data into proper Column and Row objects\n",
    "from pydantic import BaseModel, Field\n",
    "import typing as t\n",
    "\n",
    "# Create Column objects\n",
    "column_objects = []\n",
    "for col in test_data_columns:\n",
    "    column_objects.append(Column(\n",
    "        name=col[\"name\"],\n",
    "        type=col[\"type\"]\n",
    "        # id and settings will be auto-generated\n",
    "    ))\n",
    "\n",
    "# Create a mapping of column names to their IDs for creating rows\n",
    "column_map = {col.name: col.id for col in column_objects}\n",
    "\n",
    "# Create Row objects\n",
    "row_objects = []\n",
    "for row in test_data_rows:\n",
    "    cells = []\n",
    "    for key, value in row.items():\n",
    "        if key in column_map:  # Skip any extra fields not in columns\n",
    "            cells.append(RowCell(\n",
    "                data=value,\n",
    "                column_id=column_map[key]\n",
    "            ))\n",
    "    row_objects.append(Row(data=cells))\n",
    "\n",
    "# Now we can create the dataset\n",
    "dataset = await client.create_dataset_with_data(\n",
    "    project_id=TEST_PROJECT_ID,\n",
    "    name=\"Capitals Dataset\",\n",
    "    description=\"A dataset about capital cities\",\n",
    "    columns=column_objects,\n",
    "    rows=row_objects\n",
    ")\n",
    "\n",
    "print(f\"Created dataset with ID: {dataset['id']}\")\n",
    "\n",
    "# Verify the data\n",
    "columns = await client.list_dataset_columns(TEST_PROJECT_ID, dataset[\"id\"])\n",
    "print(f\"Created {len(columns['items'])} columns\")\n",
    "\n",
    "rows = await client.list_dataset_rows(TEST_PROJECT_ID, dataset[\"id\"])\n",
    "print(f\"Created {len(rows['items'])} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://dev.app.ragas.io/dashboard/projects/e1b3f1e4-d344-48f4-a178-84e7e32e6ab6/datasets/5e7912f4-6a65-4d0c-bf79-0fab9ddda40c'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get dataset url\n",
    "url = f\"{WEB_ENDPOINT}/dashboard/projects/{TEST_PROJECT_ID}/datasets/{dataset['id']}\"\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup\n",
    "await client.delete_dataset(TEST_PROJECT_ID, dataset[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The same but for Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#---- Experiment Columns ----\n",
    "@patch\n",
    "async def list_experiment_columns(\n",
    "    self: RagasRelay,\n",
    "    project_id: str,\n",
    "    experiment_id: str,\n",
    "    limit: int = 50,\n",
    "    offset: int = 0,\n",
    "    order_by: t.Optional[str] = None,\n",
    "    sort_dir: t.Optional[str] = None,\n",
    ") -> t.Dict:\n",
    "    \"\"\"List columns in an experiment.\"\"\"\n",
    "    params = {\"limit\": limit, \"offset\": offset}\n",
    "    if order_by:\n",
    "        params[\"order_by\"] = order_by\n",
    "    if sort_dir:\n",
    "        params[\"sort_dir\"] = sort_dir\n",
    "    return await self._list_resources(\n",
    "        f\"projects/{project_id}/experiments/{experiment_id}/columns\", **params\n",
    "    )\n",
    "\n",
    "@patch\n",
    "async def get_experiment_column(\n",
    "    self: RagasRelay, project_id: str, experiment_id: str, column_id: str\n",
    ") -> t.Dict:\n",
    "    \"\"\"Get a specific column in an experiment.\"\"\"\n",
    "    return await self._get_resource(\n",
    "        f\"projects/{project_id}/experiments/{experiment_id}/columns/{column_id}\"\n",
    "    )\n",
    "\n",
    "@patch\n",
    "async def create_experiment_column(\n",
    "    self: RagasRelay,\n",
    "    project_id: str,\n",
    "    experiment_id: str,\n",
    "    id: str,\n",
    "    name: str,\n",
    "    type: str,\n",
    "    col_order: t.Optional[int] = None,\n",
    "    settings: t.Optional[t.Dict] = None,\n",
    ") -> t.Dict:\n",
    "    \"\"\"Create a new column in an experiment.\"\"\"\n",
    "    data = {\"id\": id, \"name\": name, \"type\": type}\n",
    "    if col_order is not None:\n",
    "        data[\"col_order\"] = col_order\n",
    "    if settings:\n",
    "        data[\"settings\"] = settings\n",
    "    return await self._create_resource(\n",
    "        f\"projects/{project_id}/experiments/{experiment_id}/columns\", data\n",
    "    )\n",
    "\n",
    "@patch\n",
    "async def update_experiment_column(\n",
    "    self: RagasRelay, project_id: str, experiment_id: str, column_id: str, **column_data\n",
    ") -> t.Dict:\n",
    "    \"\"\"Update an existing column in an experiment.\"\"\"\n",
    "    return await self._update_resource(\n",
    "        f\"projects/{project_id}/experiments/{experiment_id}/columns/{column_id}\",\n",
    "        column_data,\n",
    "    )\n",
    "\n",
    "@patch\n",
    "async def delete_experiment_column(\n",
    "    self: RagasRelay, project_id: str, experiment_id: str, column_id: str\n",
    ") -> None:\n",
    "    \"\"\"Delete a column from an experiment.\"\"\"\n",
    "    await self._delete_resource(\n",
    "        f\"projects/{project_id}/experiments/{experiment_id}/columns/{column_id}\"\n",
    "    )\n",
    "\n",
    "#---- Experiment Rows ----\n",
    "@patch\n",
    "async def list_experiment_rows(\n",
    "    self: RagasRelay,\n",
    "    project_id: str,\n",
    "    experiment_id: str,\n",
    "    limit: int = 50,\n",
    "    offset: int = 0,\n",
    "    order_by: t.Optional[str] = None,\n",
    "    sort_dir: t.Optional[str] = None,\n",
    ") -> t.Dict:\n",
    "    \"\"\"List rows in an experiment.\"\"\"\n",
    "    params = {\"limit\": limit, \"offset\": offset}\n",
    "    if order_by:\n",
    "        params[\"order_by\"] = order_by\n",
    "    if sort_dir:\n",
    "        params[\"sort_dir\"] = sort_dir\n",
    "    return await self._list_resources(\n",
    "        f\"projects/{project_id}/experiments/{experiment_id}/rows\", **params\n",
    "    )\n",
    "\n",
    "@patch\n",
    "async def get_experiment_row(\n",
    "    self: RagasRelay, project_id: str, experiment_id: str, row_id: str\n",
    ") -> t.Dict:\n",
    "    \"\"\"Get a specific row in an experiment.\"\"\"\n",
    "    return await self._get_resource(\n",
    "        f\"projects/{project_id}/experiments/{experiment_id}/rows/{row_id}\"\n",
    "    )\n",
    "\n",
    "@patch\n",
    "async def create_experiment_row(\n",
    "    self: RagasRelay, project_id: str, experiment_id: str, id: str, data: t.Dict\n",
    ") -> t.Dict:\n",
    "    \"\"\"Create a new row in an experiment.\"\"\"\n",
    "    row_data = {\"id\": id, \"data\": data}\n",
    "    return await self._create_resource(\n",
    "        f\"projects/{project_id}/experiments/{experiment_id}/rows\", row_data\n",
    "    )\n",
    "\n",
    "@patch\n",
    "async def update_experiment_row(\n",
    "    self: RagasRelay, project_id: str, experiment_id: str, row_id: str, data: t.Dict\n",
    ") -> t.Dict:\n",
    "    \"\"\"Update an existing row in an experiment.\"\"\"\n",
    "    row_data = {\"data\": data}\n",
    "    return await self._update_resource(\n",
    "        f\"projects/{project_id}/experiments/{experiment_id}/rows/{row_id}\",\n",
    "        row_data,\n",
    "    )\n",
    "\n",
    "@patch\n",
    "async def delete_experiment_row(\n",
    "    self: RagasRelay, project_id: str, experiment_id: str, row_id: str\n",
    ") -> None:\n",
    "    \"\"\"Delete a row from an experiment.\"\"\"\n",
    "    await self._delete_resource(\n",
    "        f\"projects/{project_id}/experiments/{experiment_id}/rows/{row_id}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '7c695b58-7fc3-464c-a18b-a96e35f9684d',\n",
       " 'name': 'New Experiment',\n",
       " 'description': 'This is a new experiment',\n",
       " 'updated_at': '2025-04-09T17:03:44.340782+00:00',\n",
       " 'created_at': '2025-04-09T17:03:44.340782+00:00',\n",
       " 'version_counter': 0,\n",
       " 'project_id': 'e1b3f1e4-d344-48f4-a178-84e7e32e6ab6'}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await client.create_experiment(TEST_PROJECT_ID, \"New Experiment\", \"This is a new experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'78fd6c58-7edf-4239-93d1-4f49185d8e49'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments = await client.list_experiments(TEST_PROJECT_ID)\n",
    "EXPERIMENT_ID = experiments[\"items\"][0][\"id\"]\n",
    "EXPERIMENT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "async def create_experiment_with_data(\n",
    "    self: RagasRelay,\n",
    "    project_id: str,\n",
    "    name: str,\n",
    "    description: str,\n",
    "    columns: t.List[Column],\n",
    "    rows: t.List[Row],\n",
    "    batch_size: int = 50\n",
    ") -> t.Dict:\n",
    "    \"\"\"Create an experiment with columns and rows.\n",
    "    \n",
    "    This method creates an experiment and populates it with columns and rows in an\n",
    "    optimized way using concurrent requests.\n",
    "    \n",
    "    Args:\n",
    "        project_id: Project ID\n",
    "        name: Experiment name\n",
    "        description: Experiment description\n",
    "        columns: List of column definitions\n",
    "        rows: List of row data\n",
    "        batch_size: Number of operations to perform concurrently\n",
    "        \n",
    "    Returns:\n",
    "        The created experiment\n",
    "    \"\"\"\n",
    "    return await self._create_with_data(\n",
    "        \"experiment\", project_id, name, description, columns, rows, batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#---- Utility Methods ----\n",
    "@patch\n",
    "def create_column(\n",
    "    self: RagasRelay, \n",
    "    name: str, \n",
    "    type: str, \n",
    "    settings: t.Optional[t.Dict] = None, \n",
    "    col_order: t.Optional[int] = None,\n",
    "    id: t.Optional[str] = None\n",
    ") -> Column:\n",
    "    \"\"\"Create a Column object.\n",
    "    \n",
    "    Args:\n",
    "        name: Column name\n",
    "        type: Column type (use ColumnType enum)\n",
    "        settings: Column settings\n",
    "        col_order: Column order\n",
    "        id: Custom ID (generates one if not provided)\n",
    "        \n",
    "    Returns:\n",
    "        Column object\n",
    "    \"\"\"\n",
    "    params = {\"name\": name, \"type\": type}\n",
    "    if settings:\n",
    "        params[\"settings\"] = settings\n",
    "    if col_order is not None:\n",
    "        params[\"col_order\"] = col_order\n",
    "    if id:\n",
    "        params[\"id\"] = id\n",
    "        \n",
    "    return Column(**params)\n",
    "    \n",
    "@patch\n",
    "def create_row(\n",
    "    self: RagasRelay, \n",
    "    data: t.Dict[str, t.Any], \n",
    "    column_map: t.Dict[str, str],\n",
    "    id: t.Optional[str] = None\n",
    ") -> Row:\n",
    "    \"\"\"Create a Row object from a dictionary.\n",
    "    \n",
    "    Args:\n",
    "        data: Dictionary mapping column names to values\n",
    "        column_map: Dictionary mapping column names to column IDs\n",
    "        id: Custom ID (generates one if not provided)\n",
    "        \n",
    "    Returns:\n",
    "        Row object\n",
    "    \"\"\"\n",
    "    cells = []\n",
    "    for col_name, value in data.items():\n",
    "        if col_name in column_map:\n",
    "            cells.append(RowCell(\n",
    "                data=value,\n",
    "                column_id=column_map[col_name]\n",
    "            ))\n",
    "            \n",
    "    params = {\"data\": cells}\n",
    "    if id:\n",
    "        params[\"id\"] = id\n",
    "        \n",
    "    return Row(**params)\n",
    "    \n",
    "@patch\n",
    "def create_column_map(self: RagasRelay, columns: t.List[Column]) -> t.Dict[str, str]:\n",
    "    \"\"\"Create a mapping of column names to IDs.\n",
    "    \n",
    "    Args:\n",
    "        columns: List of column objects\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping column names to IDs\n",
    "    \"\"\"\n",
    "    return {col.name: col.id for col in columns}\n",
    "    \n",
    "@patch\n",
    "async def convert_raw_data(\n",
    "    self: RagasRelay,\n",
    "    column_defs: t.List[t.Dict],\n",
    "    row_data: t.List[t.Dict]\n",
    ") -> t.Tuple[t.List[Column], t.List[Row]]:\n",
    "    \"\"\"Convert raw data to column and row objects.\n",
    "    \n",
    "    Args:\n",
    "        column_defs: List of column definitions (dicts with name, type)\n",
    "        row_data: List of dictionaries with row data\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (columns, rows)\n",
    "    \"\"\"\n",
    "    # Create columns\n",
    "    columns = []\n",
    "    for col in column_defs:\n",
    "        columns.append(self.create_column(**col))\n",
    "        \n",
    "    # Create column map\n",
    "    column_map = self.create_column_map(columns)\n",
    "    \n",
    "    # Create rows\n",
    "    rows = []\n",
    "    for data in row_data:\n",
    "        rows.append(self.create_row(data, column_map))\n",
    "        \n",
    "    return columns, rows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
