{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp llm.llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Interface for Ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/random/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "import typing as t\n",
    "import asyncio\n",
    "import inspect\n",
    "from pydantic import BaseModel\n",
    "import instructor\n",
    "\n",
    "T = t.TypeVar('T', bound=BaseModel)\n",
    "\n",
    "class RagasLLM:\n",
    "    def __init__(self, provider: str, model:str, client: t.Any):\n",
    "        self.provider = provider.lower()\n",
    "        self.model = model\n",
    "        self.client = self._initialize_client(provider, client)\n",
    "        # Check if client is async-capable at initialization\n",
    "        self.is_async = self._check_client_async()\n",
    "    \n",
    "    def _check_client_async(self) -> bool:\n",
    "        \"\"\"Determine if the client is async-capable.\"\"\"\n",
    "        try:\n",
    "            # Check if this is an async client by checking for a coroutine method\n",
    "            if hasattr(self.client.chat.completions, 'create'):\n",
    "                return inspect.iscoroutinefunction(self.client.chat.completions.create)\n",
    "            return False\n",
    "        except (AttributeError, TypeError):\n",
    "            return False\n",
    "    \n",
    "    def _initialize_client(self, provider: str, client: t.Any) -> t.Any:\n",
    "        provider = provider.lower()\n",
    "        \n",
    "        if provider == \"openai\":\n",
    "            return instructor.from_openai(client)\n",
    "        elif provider == \"anthropic\":\n",
    "            return instructor.from_anthropic(client)\n",
    "        elif provider == \"cohere\":\n",
    "            return instructor.from_cohere(client)\n",
    "        elif provider == \"gemini\":\n",
    "            return instructor.from_gemini(client)\n",
    "        elif provider == \"litellm\":\n",
    "            return instructor.from_litellm(client)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported provider: {provider}\")\n",
    "    \n",
    "    def _run_async_in_current_loop(self, coro):\n",
    "        \"\"\"Run an async coroutine in the current event loop if possible.\n",
    "        \n",
    "        This handles Jupyter environments correctly by using the existing loop.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check if we're in an environment with an existing event loop (like Jupyter)\n",
    "            loop = asyncio.get_event_loop()\n",
    "            if loop.is_running():\n",
    "                # We're likely in a Jupyter environment\n",
    "                import nest_asyncio\n",
    "                nest_asyncio.apply()\n",
    "            return loop.run_until_complete(coro)\n",
    "        except RuntimeError:\n",
    "            # If we get a runtime error about no event loop, create a new one\n",
    "            loop = asyncio.new_event_loop()\n",
    "            asyncio.set_event_loop(loop)\n",
    "            try:\n",
    "                return loop.run_until_complete(coro)\n",
    "            finally:\n",
    "                loop.close()\n",
    "                asyncio.set_event_loop(None)\n",
    "    \n",
    "    def generate(self, prompt: str, response_model: t.Type[T], **kwargs) -> T:\n",
    "        \"\"\"Generate a response using the configured LLM.\n",
    "        \n",
    "        For async clients, this will run the async method in the appropriate event loop.\n",
    "        \"\"\"\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        if \"model\" not in kwargs and self.model:\n",
    "            kwargs[\"model\"] = self.model\n",
    "        \n",
    "        # If client is async, use the appropriate method to run it\n",
    "        if self.is_async:\n",
    "            return self._run_async_in_current_loop(\n",
    "                self.agenerate(prompt, response_model, **kwargs)\n",
    "            )\n",
    "        else:\n",
    "            # Regular sync client, just call the method directly\n",
    "            return self.client.chat.completions.create(\n",
    "                messages=messages,\n",
    "                response_model=response_model,\n",
    "                **kwargs\n",
    "            )\n",
    "    \n",
    "    async def agenerate(self, prompt: str, response_model: t.Type[T], **kwargs) -> T:\n",
    "        \"\"\"Asynchronously generate a response using the configured LLM.\"\"\"\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        if \"model\" not in kwargs and self.model:\n",
    "            kwargs[\"model\"] = self.model\n",
    "        \n",
    "        # If client is not async, raise a helpful error\n",
    "        if not self.is_async:\n",
    "            raise TypeError(\n",
    "                \"Cannot use agenerate() with a synchronous client. Use generate() instead.\"\n",
    "            )\n",
    "        \n",
    "        # Regular async client, call the method directly\n",
    "        return await self.client.chat.completions.create(\n",
    "            messages=messages,\n",
    "            response_model=response_model,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "def ragas_llm(provider: str,model:str, client: t.Any,) -> RagasLLM:\n",
    "    return RagasLLM(provider=provider, client=client, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "from openai import OpenAI\n",
    "class Response(BaseModel):\n",
    "    response: str\n",
    "\n",
    "llm = ragas_llm(provider=\"openai\",model=\"gpt-4o\",client=OpenAI())\n",
    "llm.generate(\"What is the capital of India?\",response_model=Response) #works fine\n",
    "\n",
    "try:\n",
    "    await llm.agenerate(\"What is the capital of India?\", response_model=Response)\n",
    "except TypeError as e:\n",
    "    assert isinstance(e, TypeError)\n",
    "#gives TypeError: object Response can't be used in 'await' expression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(response='The capital of India is New Delhi.')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| eval: false\n",
    "\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "llm = ragas_llm(provider=\"openai\",model=\"gpt-4o\",client=AsyncOpenAI())\n",
    "await llm.agenerate(\"What is the capital of India?\",response_model=Response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
