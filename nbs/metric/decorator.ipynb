{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp metric.decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decorator factory for metrics\n",
    "> decorator factory for creating custom metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/Users/shahules/Myprojects/ragas_annotator/.envrc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import typing as t\n",
    "import inspect\n",
    "import asyncio\n",
    "from dataclasses import dataclass\n",
    "from ragas_annotator.metric import MetricResult\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_metric_decorator(metric_class):\n",
    "    \"\"\"\n",
    "    Factory function that creates decorator factories for different metric types.\n",
    "    \n",
    "    Args:\n",
    "        metric_class: The metric class to use (DiscreteMetrics, NumericMetrics, etc.)\n",
    "        \n",
    "    Returns:\n",
    "        A decorator factory function for the specified metric type\n",
    "    \"\"\"\n",
    "    def decorator_factory(llm, prompt, name: t.Optional[str] = None, **metric_params):\n",
    "        \"\"\"\n",
    "        Creates a decorator that wraps a function into a metric instance.\n",
    "        \n",
    "        Args:\n",
    "            llm: The language model instance to use\n",
    "            prompt: The prompt template\n",
    "            name: Optional name for the metric (defaults to function name)\n",
    "            **metric_params: Additional parameters specific to the metric type\n",
    "                (values for DiscreteMetrics, range for NumericMetrics, etc.)\n",
    "        \n",
    "        Returns:\n",
    "            A decorator function\n",
    "        \"\"\"\n",
    "        def decorator(func):\n",
    "            # Get metric name and check if function is async\n",
    "            metric_name = name or func.__name__\n",
    "            is_async = inspect.iscoroutinefunction(func)\n",
    "            \n",
    "            @dataclass\n",
    "            class CustomMetric(metric_class):\n",
    "                def _extract_result(self, result, reasoning: bool):\n",
    "                    \"\"\"Extract score and reason from the result.\"\"\"\n",
    "                    if isinstance(result, tuple) and len(result) == 2:\n",
    "                        score, reason = result\n",
    "                    else:\n",
    "                        score, reason = result, None\n",
    "                    \n",
    "                    # Use \"result\" instead of \"score\" for the new MetricResult implementation\n",
    "                    return MetricResult(result=score, reason=reason if reasoning else None)\n",
    "                \n",
    "                def _run_sync_in_async(self, func, *args, **kwargs):\n",
    "                    \"\"\"Run a synchronous function in an async context.\"\"\"\n",
    "                    # For sync functions, just run them normally\n",
    "                    return func(*args, **kwargs)\n",
    "                \n",
    "                def _execute_metric(self, is_async_execution, reasoning, **kwargs):\n",
    "                    \"\"\"Execute the metric function with proper async handling.\"\"\"\n",
    "                    try:\n",
    "                        if is_async:\n",
    "                            # Async function implementation\n",
    "                            if is_async_execution:\n",
    "                                # In async context, await the function directly\n",
    "                                result = func(self.llm, self.prompt, **kwargs)\n",
    "                            else:\n",
    "                                # In sync context, run the async function in an event loop\n",
    "                                try:\n",
    "                                    loop = asyncio.get_event_loop()\n",
    "                                except RuntimeError:\n",
    "                                    loop = asyncio.new_event_loop()\n",
    "                                    asyncio.set_event_loop(loop)\n",
    "                                result = loop.run_until_complete(func(self.llm, self.prompt, **kwargs))\n",
    "                        else:\n",
    "                            # Sync function implementation\n",
    "                            result = func(self.llm, self.prompt, **kwargs)\n",
    "                        \n",
    "                        return self._extract_result(result, reasoning)\n",
    "                    except Exception as e:\n",
    "                        # Handle errors gracefully\n",
    "                        error_msg = f\"Error executing metric {self.name}: {str(e)}\"\n",
    "                        return MetricResult(result=None, reason=error_msg)\n",
    "                \n",
    "                def score(self, reasoning: bool = True, n: int = 1, **kwargs):\n",
    "                    \"\"\"Synchronous scoring method.\"\"\"\n",
    "                    return self._execute_metric(is_async_execution=False, reasoning=reasoning, **kwargs)\n",
    "                \n",
    "                async def ascore(self, reasoning: bool = True, n: int = 1, **kwargs):\n",
    "                    \"\"\"Asynchronous scoring method.\"\"\"\n",
    "                    if is_async:\n",
    "                        # For async functions, await the result\n",
    "                        result = await func(self.llm, self.prompt, **kwargs)\n",
    "                        return self._extract_result(result, reasoning)\n",
    "                    else:\n",
    "                        # For sync functions, run normally\n",
    "                        result = self._run_sync_in_async(func, self.llm, self.prompt, **kwargs)\n",
    "                        return self._extract_result(result, reasoning)\n",
    "            \n",
    "            # Create the metric instance with all parameters\n",
    "            metric_instance = CustomMetric(\n",
    "                name=metric_name,\n",
    "                prompt=prompt,\n",
    "                llm=llm,\n",
    "                **metric_params\n",
    "            )\n",
    "            \n",
    "            # Preserve metadata\n",
    "            metric_instance.__name__ = metric_name\n",
    "            metric_instance.__doc__ = func.__doc__\n",
    "            \n",
    "            return metric_instance\n",
    "        \n",
    "        return decorator\n",
    "    \n",
    "    return decorator_factory\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low\n",
      "reason\n"
     ]
    }
   ],
   "source": [
    "### Example usage\n",
    "from ragas_annotator.metric import DiscreteMetric\n",
    "from ragas_annotator.metric.llm import LLM\n",
    "from pydantic import BaseModel\n",
    "\n",
    "discrete_metric = create_metric_decorator(DiscreteMetric)\n",
    "\n",
    "@discrete_metric(llm=LLM(),\n",
    "    prompt=\"Evaluate if given answer is helpful\\n\\n{response}\",\n",
    "    name='new_metric',values=[\"low\",\"med\",\"high\"])\n",
    "def my_metric(llm,prompt,**kwargs):\n",
    "\n",
    "        class response_model(BaseModel):\n",
    "             output: t.List[bool]\n",
    "             reason: str\n",
    "        \n",
    "        response = llm.generate(prompt.format(**kwargs),response_model=response_model)\n",
    "        total = sum(response.output)\n",
    "        if total < 1:\n",
    "            score = 'low'\n",
    "        else:\n",
    "            score = 'high'\n",
    "        return score,\"reason\"\n",
    "\n",
    "result = my_metric.score(response='my response') # result\n",
    "print(result)\n",
    "print(result.reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "random",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
