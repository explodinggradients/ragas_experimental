{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RankingMetric\n",
    "> Base class for ranking metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp metric.ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import typing as t\n",
    "from dataclasses import dataclass\n",
    "from pydantic import BaseModel, Field\n",
    "from ragas_annotator.metric import Metric, MetricResult\n",
    "from ragas_annotator.metric.decorator import create_metric_decorator\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RankingMetric(Metric):\n",
    "    num_ranks: int\n",
    "\n",
    "    def _get_response_model(self, with_reasoning: bool) -> t.Type[BaseModel]:\n",
    "        \"\"\"Get or create a response model based on reasoning parameter.\"\"\"\n",
    "\n",
    "        if with_reasoning in self._response_models:\n",
    "            return self._response_models[with_reasoning]\n",
    "\n",
    "        # Store values needed for validation\n",
    "        num_ranks = self.num_ranks\n",
    "\n",
    "        # Create explicit model classes instead of using create_model\n",
    "        if with_reasoning:\n",
    "            # Model with result and reason\n",
    "            class ResponseModelWithReason(BaseModel):\n",
    "                result: t.List[int] = Field(...)\n",
    "                reason: str = Field(...)\n",
    "\n",
    "                def model_post_init(self, __context):\n",
    "                    expected = set(range(num_ranks))\n",
    "                    if set(self.result) != expected:\n",
    "                        raise ValueError(\n",
    "                            f\"'result' must contain exactly the numbers {sorted(expected)} without repetition.\"\n",
    "                        )\n",
    "\n",
    "            self._response_models[with_reasoning] = ResponseModelWithReason\n",
    "            return ResponseModelWithReason\n",
    "        else:\n",
    "            # Model with just result\n",
    "            class ResponseModel(BaseModel):\n",
    "                result: t.List[int] = Field(...)\n",
    "\n",
    "                def model_post_init(self, __context):\n",
    "                    expected = set(range(num_ranks))\n",
    "                    if set(self.result) != expected:\n",
    "                        raise ValueError(\n",
    "                            f\"'result' must contain exactly the numbers {sorted(expected)} without repetition.\"\n",
    "                        )\n",
    "\n",
    "            self._response_models[with_reasoning] = ResponseModel\n",
    "            return ResponseModel\n",
    "\n",
    "    def _ensemble(self, results: t.List[MetricResult]) -> MetricResult:\n",
    "        if len(results) == 1:\n",
    "            return results[0]\n",
    "\n",
    "        n_items = self.num_ranks  # Use the class attribute instead of len(results)\n",
    "        borda_scores = [0] * n_items\n",
    "\n",
    "        for result in results:\n",
    "            for position_idx, item_idx in enumerate(result.result):\n",
    "                borda_scores[item_idx] += n_items - position_idx  # Fixed the formula\n",
    "\n",
    "        indexed_scores = [(score, i) for i, score in enumerate(borda_scores)]\n",
    "        indexed_scores.sort(key=lambda x: (-x[0], x[1]))\n",
    "        final_ranking = [pos for _, pos in indexed_scores]\n",
    "\n",
    "        if any(r.reason for r in results):\n",
    "            reason = \"Ensemble ranking based on multiple evaluations.\\n\" + \"\\n\".join(\n",
    "                [r.reason for r in results if r.reason]\n",
    "            )\n",
    "        else:\n",
    "            reason = None\n",
    "\n",
    "        return MetricResult(result=final_ranking, reason=reason)\n",
    "\n",
    "\n",
    "ranking_metric = create_metric_decorator(RankingMetric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n",
      "Ensemble ranking based on multiple evaluations.\n",
      "The ranking is based on the length and detail of the responses, with 'short answer.' being the least detailed (rank 0), 'a bit more detailed.' being moderate (rank 1), and 'the longest and most detailed answer.' being the most comprehensive (rank 2).\n",
      "The ranking is based on the length and detail of the responses. The shortest response is ranked the lowest (0), the moderately detailed response is ranked higher (1), and the longest and most detailed response is ranked the highest (2).\n",
      "Ranking is based on length and detail; the longest answer (2) is most detailed, followed by a bit more detailed (1), and the shortest answer (0) is the least detailed.\n"
     ]
    }
   ],
   "source": [
    "# | eval: false\n",
    "\n",
    "from ragas_annotator.metric.llm import LLM\n",
    "\n",
    "my_ranking_metric = RankingMetric(\n",
    "    name=\"response_ranking\",\n",
    "    llm=LLM(),  # Your language model instance\n",
    "    prompt=\"Rank the following responses:\\n{candidates}\",\n",
    "    num_ranks=3,\n",
    ")\n",
    "\n",
    "# To score a single input (ranking candidate responses)\n",
    "result = my_ranking_metric.score(\n",
    "    candidates=[\n",
    "        \"short answer.\",\n",
    "        \"a bit more detailed.\",\n",
    "        \"the longest and most detailed answer.\",\n",
    "    ],\n",
    "    n=3,\n",
    ")\n",
    "print(result)  # Might output something like: [1, 0, 2]\n",
    "print(result.reason)  # Provides the reasoning behind the ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom ranking metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2]\n",
      "Ranked based on response clarity and detail.\n"
     ]
    }
   ],
   "source": [
    "# | eval: false\n",
    "\n",
    "\n",
    "@ranking_metric(\n",
    "    llm=LLM(),  # Your language model instance\n",
    "    prompt=\"Rank the following responses:\\n{candidates}\",\n",
    "    name=\"new_ranking_metric\",\n",
    "    num_ranks=3,\n",
    ")\n",
    "def my_ranking_metric(llm, prompt, **kwargs):\n",
    "    # Your custom logic that calls the LLM and returns a tuple of (ranking, reason)\n",
    "    # For example, process the prompt (formatted with candidates) and produce a ranking.\n",
    "    ranking = [\n",
    "        1,\n",
    "        0,\n",
    "        2,\n",
    "    ]  # Dummy ranking: second candidate is best, then first, then third.\n",
    "    reason = \"Ranked based on response clarity and detail.\"\n",
    "    return ranking, reason\n",
    "\n",
    "\n",
    "# Using the decorator-based ranking metric:\n",
    "result = my_ranking_metric.score(\n",
    "    candidates=[\n",
    "        \"Response A: short answer.\",\n",
    "        \"Response B: a bit more detailed.\",\n",
    "        \"Response C: the longest and most detailed answer.\",\n",
    "    ]\n",
    ")\n",
    "print(result)  # E.g., [1, 0, 2]\n",
    "print(result.reason)  # E.g., \"Ranked based on response clarity and detail.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
