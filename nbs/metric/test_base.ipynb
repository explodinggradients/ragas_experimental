{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c48aac0f-c63c-4bfb-95b0-a1239f41ccb3",
   "metadata": {},
   "source": [
    "# Test Base\n",
    "\n",
    "### Do not export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d14fc66-b8af-4a75-b761-20b8e9ce19f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/Users/shahules/Myprojects/ragas_annotator/.envrc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b05c525-e153-49ab-b768-5069f624f215",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/random/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import typing as t\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "from abc import ABC, abstractmethod\n",
    "import asyncio\n",
    "from dataclasses import dataclass\n",
    "from pydantic import BaseModel\n",
    "import openai\n",
    "import instructor\n",
    "from dataclasses import dataclass, field\n",
    "from pydantic import BaseModel, create_model\n",
    "import typing as t\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be7f588f-2a13-4c10-9775-00101a03e0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/random/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import instructor\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class LLM:\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.aclient = instructor.from_openai(openai.AsyncOpenAI())\n",
    "        self.client = instructor.from_openai(openai.OpenAI())\n",
    "\n",
    "    \n",
    "    def generate(self,prompt,response_model):\n",
    "        return self.client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        response_model=response_model,\n",
    "    )\n",
    "\n",
    "    async def agenerate(self,prompt,response_model):\n",
    "        return await self.aclient.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        response_model=response_model,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfed36e",
   "metadata": {},
   "source": [
    "## MetricResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca623dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import typing as t\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MetricResult:\n",
    "    \"\"\"Class to hold the result of a metric evaluation.\n",
    "    \n",
    "    This class behaves like its underlying result value but still provides access\n",
    "    to additional metadata like reasoning.\n",
    "    \n",
    "    Works with:\n",
    "    - DiscreteMetrics (string results)\n",
    "    - NumericMetrics (float/int results)\n",
    "    - RankingMetrics (list results)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, result: t.Any, reason: t.Optional[str] = None):\n",
    "        self._result = result\n",
    "        self.reason = reason\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return repr(self._result)\n",
    "    \n",
    "    # Access to underlying result\n",
    "    @property\n",
    "    def result(self):\n",
    "        \"\"\"Get the raw result value.\"\"\"\n",
    "        return self._result\n",
    "    \n",
    "    \n",
    "    # String conversion - works for all types\n",
    "    def __str__(self):\n",
    "        return str(self._result)\n",
    "    \n",
    "    # Container-like behaviors for list results (RankingMetric)\n",
    "    def __getitem__(self, key):\n",
    "        if not hasattr(self._result, \"__getitem__\"):\n",
    "            raise TypeError(f\"{type(self._result).__name__} object is not subscriptable\")\n",
    "        return self._result[key]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if not hasattr(self._result, \"__iter__\"):\n",
    "            raise TypeError(f\"{type(self._result).__name__} object is not iterable\")\n",
    "        return iter(self._result)\n",
    "    \n",
    "    def __len__(self):\n",
    "        if not hasattr(self._result, \"__len__\"):\n",
    "            raise TypeError(f\"{type(self._result).__name__} has no len()\")\n",
    "        return len(self._result)\n",
    "    \n",
    "    # Numeric operations for numeric results (NumericMetric)\n",
    "    def __float__(self):\n",
    "        if isinstance(self._result, (int, float)):\n",
    "            return float(self._result)\n",
    "        raise TypeError(f\"Cannot convert {type(self._result).__name__} to float\")\n",
    "    \n",
    "    def __int__(self):\n",
    "        if isinstance(self._result, (int, float)):\n",
    "            return int(self._result)\n",
    "        raise TypeError(f\"Cannot convert {type(self._result).__name__} to int\")\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        if not isinstance(self._result, (int, float)):\n",
    "            raise TypeError(f\"Cannot add {type(self._result).__name__} objects\")\n",
    "        if isinstance(other, MetricResult):\n",
    "            return self._result + other._result\n",
    "        return self._result + other\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        if not isinstance(self._result, (int, float)):\n",
    "            raise TypeError(f\"Cannot add {type(self._result).__name__} objects\")\n",
    "        return other + self._result\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if not isinstance(self._result, (int, float)):\n",
    "            raise TypeError(f\"Cannot subtract {type(self._result).__name__} objects\")\n",
    "        if isinstance(other, MetricResult):\n",
    "            return self._result - other._result\n",
    "        return self._result - other\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        if not isinstance(self._result, (int, float)):\n",
    "            raise TypeError(f\"Cannot subtract {type(self._result).__name__} objects\")\n",
    "        return other - self._result\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if not isinstance(self._result, (int, float)):\n",
    "            raise TypeError(f\"Cannot multiply {type(self._result).__name__} objects\")\n",
    "        if isinstance(other, MetricResult):\n",
    "            return self._result * other._result\n",
    "        return self._result * other\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        if not isinstance(self._result, (int, float)):\n",
    "            raise TypeError(f\"Cannot multiply {type(self._result).__name__} objects\")\n",
    "        return other * self._result\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        if not isinstance(self._result, (int, float)):\n",
    "            raise TypeError(f\"Cannot divide {type(self._result).__name__} objects\")\n",
    "        if isinstance(other, MetricResult):\n",
    "            return self._result / other._result\n",
    "        return self._result / other\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "        if not isinstance(self._result, (int, float)):\n",
    "            raise TypeError(f\"Cannot divide {type(self._result).__name__} objects\")\n",
    "        return other / self._result\n",
    "    \n",
    "    # Comparison operations - work for all types with same-type comparisons\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, MetricResult):\n",
    "            return self._result == other._result\n",
    "        return self._result == other\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        if isinstance(other, MetricResult):\n",
    "            return self._result < other._result\n",
    "        return self._result < other\n",
    "    \n",
    "    def __le__(self, other):\n",
    "        if isinstance(other, MetricResult):\n",
    "            return self._result <= other._result\n",
    "        return self._result <= other\n",
    "    \n",
    "    def __gt__(self, other):\n",
    "        if isinstance(other, MetricResult):\n",
    "            return self._result > other._result\n",
    "        return self._result > other\n",
    "    \n",
    "    def __ge__(self, other):\n",
    "        if isinstance(other, MetricResult):\n",
    "            return self._result >= other._result\n",
    "        return self._result >= other\n",
    "    \n",
    "    # Method forwarding for type-specific behaviors\n",
    "    def __getattr__(self, name):\n",
    "        \"\"\"Forward attribute access to the result object if it has that attribute.\n",
    "        \n",
    "        This allows calling string methods on discrete results, \n",
    "        numeric methods on numeric results, and list methods on ranking results.\n",
    "        \"\"\"\n",
    "        if hasattr(self._result, name):\n",
    "            attr = getattr(self._result, name)\n",
    "            if callable(attr):\n",
    "                # If it's a method, wrap it to return MetricResult when appropriate\n",
    "                def wrapper(*args, **kwargs):\n",
    "                    result = attr(*args, **kwargs)\n",
    "                    # If the result is of the same type as self._result, wrap it\n",
    "                    if isinstance(result, type(self._result)):\n",
    "                        return MetricResult(result=result, reason=self.reason)\n",
    "                    return result\n",
    "                return wrapper\n",
    "            return attr\n",
    "        raise AttributeError(f\"{type(self).__name__} has no attribute '{name}'\")\n",
    "    \n",
    "    # JSON/dict serialization\n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert the result to a dictionary.\"\"\"\n",
    "        return {\n",
    "            \"result\": self._result,\n",
    "            \"reason\": self.reason\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bec7946-c61c-4631-9880-fff575974e39",
   "metadata": {},
   "source": [
    "### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e19478-d947-405b-a229-4a1e7daa2fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "import asyncio\n",
    "from dataclasses import dataclass, field\n",
    "from pydantic import BaseModel\n",
    "import typing as t\n",
    "from ragas_annotator.metric import MetricResult\n",
    "\n",
    "@dataclass\n",
    "class Metric(ABC):\n",
    "    \"\"\"Base class for all metrics in the LLM evaluation library.\"\"\"\n",
    "    name: str\n",
    "    prompt: str\n",
    "    llm: 'LLM'  # Forward reference with quotes\n",
    "    _response_models: t.Dict[bool, t.Type[BaseModel]] = field(\n",
    "        default_factory=dict, init=False, repr=False\n",
    "    )\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _get_response_model(self, with_reasoning: bool) -> t.Type[BaseModel]:\n",
    "        \"\"\"Get the appropriate response model.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _ensemble(self, results: t.List[MetricResult]) -> MetricResult:\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    def score(self, reasoning: bool = True, n: int = 1, **kwargs) -> t.Any:\n",
    "        responses = []\n",
    "        prompt_input = self.prompt.format(**kwargs)\n",
    "        for _ in range(n):\n",
    "            response = self.llm.generate(prompt_input, response_model = self._get_response_model(reasoning)) \n",
    "            response = MetricResult(**response.model_dump())\n",
    "            responses.append(response)\n",
    "        return self._ensemble(responses)\n",
    "\n",
    "\n",
    "    async def ascore(self, reasoning: bool = True, n: int = 1, **kwargs) -> MetricResult:\n",
    "        responses = []  # Added missing initialization\n",
    "        prompt_input = self.prompt.format(**kwargs)\n",
    "        for _ in range(n):\n",
    "            response = await self.llm.agenerate(prompt_input, response_model = self._get_response_model(reasoning))\n",
    "            response = MetricResult(**response.model_dump())  # Fixed missing parentheses\n",
    "            responses.append(response)\n",
    "        return self._ensemble(responses)\n",
    "        \n",
    "    def batch_score(self, inputs: t.List[t.Dict[str, t.Any]], reasoning: bool = True, n: int = 1) -> t.List[t.Any]:\n",
    "        return [self.score(reasoning, n, **input_dict) for input_dict in inputs]\n",
    "    \n",
    "    async def abatch_score(self, inputs: t.List[t.Dict[str, t.Any]], reasoning: bool = True, n: int = 1) -> t.List[MetricResult]:\n",
    "        async_tasks = []\n",
    "        for input_dict in inputs:\n",
    "            # Add reasoning and n to the input parameters\n",
    "            async_tasks.append(self.ascore(reasoning=reasoning, n=n, **input_dict))\n",
    "            \n",
    "        # Run all tasks concurrently and return results\n",
    "        return await asyncio.gather(*async_tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af79bb4-4c3e-4004-b7f3-ec36e50b4ca5",
   "metadata": {},
   "source": [
    "### DiscreteMetric \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09683e0-5ec3-4e60-a8c4-1657e2fe60b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from collections import Counter\n",
    "\n",
    "@dataclass\n",
    "class DiscreteMetrics(Metric):\n",
    "    values: t.List[str] = field(default_factory=lambda: [\"pass\", \"fail\"])\n",
    "    \n",
    "    def _get_response_model(self, with_reasoning: bool) -> t.Type[BaseModel]:\n",
    "        \"\"\"Get or create a response model based on reasoning parameter.\"\"\"\n",
    "        \n",
    "        if with_reasoning in self._response_models:\n",
    "            return self._response_models[with_reasoning]\n",
    "        \n",
    "        model_name = 'response_model'\n",
    "        values = tuple(self.values)\n",
    "        fields = {\"result\": (t.Literal[values], ...)}\n",
    "        \n",
    "        if with_reasoning:\n",
    "            fields[\"reason\"] = (str, ...)\n",
    "        \n",
    "        model = create_model(model_name, **fields)\n",
    "        self._response_models[with_reasoning] = model\n",
    "        return model \n",
    "\n",
    "    def _ensemble(self,results:t.List[MetricResult]) -> MetricResult:\n",
    "\n",
    "\n",
    "        if len(results)==1:\n",
    "            return results[0]\n",
    "            \n",
    "        candidates = [candidate.result for candidate in results]\n",
    "        counter = Counter(candidates)\n",
    "        max_count = max(counter.values())\n",
    "        for candidate in results:\n",
    "            if counter[candidate.result] == max_count:\n",
    "                result = candidate.result              \n",
    "                reason = candidate.reason\n",
    "                break\n",
    "        \n",
    "        return MetricResult(result=result,reason=reason)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33ef89f-4ccc-4307-9944-4f372ce77830",
   "metadata": {},
   "source": [
    "### decorator factory for discrete_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "99d5afd6-72bd-42d7-bff0-effce9cf8cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as t\n",
    "from typing import Any, Callable, Dict, List, Optional, Type, Union\n",
    "import inspect\n",
    "import asyncio\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC\n",
    "\n",
    "def create_metric_decorator(metric_class):\n",
    "    \"\"\"\n",
    "    Factory function that creates decorator factories for different metric types.\n",
    "    \n",
    "    Args:\n",
    "        metric_class: The metric class to use (DiscreteMetrics, NumericMetrics, etc.)\n",
    "        \n",
    "    Returns:\n",
    "        A decorator factory function for the specified metric type\n",
    "    \"\"\"\n",
    "    def decorator_factory(llm, prompt, name: t.Optional[str] = None, **metric_params):\n",
    "        \"\"\"\n",
    "        Creates a decorator that wraps a function into a metric instance.\n",
    "        \n",
    "        Args:\n",
    "            llm: The language model instance to use\n",
    "            prompt: The prompt template\n",
    "            name: Optional name for the metric (defaults to function name)\n",
    "            **metric_params: Additional parameters specific to the metric type\n",
    "                (values for DiscreteMetrics, range for NumericMetrics, etc.)\n",
    "        \n",
    "        Returns:\n",
    "            A decorator function\n",
    "        \"\"\"\n",
    "        def decorator(func):\n",
    "            # Get metric name and check if function is async\n",
    "            metric_name = name or func.__name__\n",
    "            is_async = inspect.iscoroutinefunction(func)\n",
    "            \n",
    "            @dataclass\n",
    "            class CustomMetric(metric_class):\n",
    "                def _extract_result(self, result, reasoning: bool):\n",
    "                    \"\"\"Extract score and reason from the result.\"\"\"\n",
    "                    if isinstance(result, tuple) and len(result) == 2:\n",
    "                        score, reason = result\n",
    "                    else:\n",
    "                        score, reason = result, None\n",
    "                    \n",
    "                    # Use \"result\" instead of \"score\" for the new MetricResult implementation\n",
    "                    return MetricResult(result=score, reason=reason if reasoning else None)\n",
    "                \n",
    "                def _run_sync_in_async(self, func, *args, **kwargs):\n",
    "                    \"\"\"Run a synchronous function in an async context.\"\"\"\n",
    "                    # For sync functions, just run them normally\n",
    "                    return func(*args, **kwargs)\n",
    "                \n",
    "                def _execute_metric(self, is_async_execution, reasoning, **kwargs):\n",
    "                    \"\"\"Execute the metric function with proper async handling.\"\"\"\n",
    "                    try:\n",
    "                        if is_async:\n",
    "                            # Async function implementation\n",
    "                            if is_async_execution:\n",
    "                                # In async context, await the function directly\n",
    "                                result = func(self.llm, self.prompt, **kwargs)\n",
    "                            else:\n",
    "                                # In sync context, run the async function in an event loop\n",
    "                                try:\n",
    "                                    loop = asyncio.get_event_loop()\n",
    "                                except RuntimeError:\n",
    "                                    loop = asyncio.new_event_loop()\n",
    "                                    asyncio.set_event_loop(loop)\n",
    "                                result = loop.run_until_complete(func(self.llm, self.prompt, **kwargs))\n",
    "                        else:\n",
    "                            # Sync function implementation\n",
    "                            result = func(self.llm, self.prompt, **kwargs)\n",
    "                        \n",
    "                        return self._extract_result(result, reasoning)\n",
    "                    except Exception as e:\n",
    "                        # Handle errors gracefully\n",
    "                        error_msg = f\"Error executing metric {self.name}: {str(e)}\"\n",
    "                        return MetricResult(result=None, reason=error_msg)\n",
    "                \n",
    "                def score(self, reasoning: bool = True, n: int = 1, **kwargs):\n",
    "                    \"\"\"Synchronous scoring method.\"\"\"\n",
    "                    return self._execute_metric(is_async_execution=False, reasoning=reasoning, **kwargs)\n",
    "                \n",
    "                async def ascore(self, reasoning: bool = True, n: int = 1, **kwargs):\n",
    "                    \"\"\"Asynchronous scoring method.\"\"\"\n",
    "                    if is_async:\n",
    "                        # For async functions, await the result\n",
    "                        result = await func(self.llm, self.prompt, **kwargs)\n",
    "                        return self._extract_result(result, reasoning)\n",
    "                    else:\n",
    "                        # For sync functions, run normally\n",
    "                        result = self._run_sync_in_async(func, self.llm, self.prompt, **kwargs)\n",
    "                        return self._extract_result(result, reasoning)\n",
    "            \n",
    "            # Create the metric instance with all parameters\n",
    "            metric_instance = CustomMetric(\n",
    "                name=metric_name,\n",
    "                prompt=prompt,\n",
    "                llm=llm,\n",
    "                **metric_params\n",
    "            )\n",
    "            \n",
    "            # Preserve metadata\n",
    "            metric_instance.__name__ = metric_name\n",
    "            metric_instance.__doc__ = func.__doc__\n",
    "            \n",
    "            return metric_instance\n",
    "        \n",
    "        return decorator\n",
    "    \n",
    "    return decorator_factory\n",
    "\n",
    "# Create specific decorator factories for each metric type\n",
    "discrete_metric = create_metric_decorator(DiscreteMetrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f49ad3-4476-4bcc-ac34-a87fb7a8652a",
   "metadata": {},
   "source": [
    "### Usage pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "aeae8fe5-e81a-44ac-9ad7-a240655a0f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The answer provided lacks specific context or detail needed to evaluate its helpfulness fully. Without more information, it's difficult to determine its applicability.\""
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "my_metric = DiscreteMetrics(\n",
    "    name='helpfulness',\n",
    "    llm=LLM(),\n",
    "    prompt=\"Evaluate if given answer is helpful\\n\\n{response}\",\n",
    "    values=[\"low\",\"med\",\"high\"],\n",
    ")\n",
    "\n",
    "result = my_metric.score(response=\"this is my response\")\n",
    "result #gives \"low\"\n",
    "result.reason #gives reasoning from llm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b5e499d8-8258-46ce-b719-0389d3cfd8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'low'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## score without reasoning to save reasoning tokens cost\n",
    "result = my_metric.score(response=\"this is my response\",reasoning=False,n=3)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a9a5d6c6-4cfc-4f45-8b19-996315a95370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reason'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@discrete_metric(llm=LLM(),\n",
    "    prompt=\"Evaluate if given answer is helpful\\n\\n{response}\",\n",
    "    name='new_metric',values=[\"low\",\"med\",\"high\"])\n",
    "def my_metric(llm,prompt,**kwargs):\n",
    "\n",
    "        class response_model(BaseModel):\n",
    "             output: t.List[bool]\n",
    "             reason: str\n",
    "        \n",
    "        response = llm.generate(prompt.format(**kwargs),response_model=response_model)\n",
    "        total = sum(response.output)\n",
    "        if total < 1:\n",
    "            score = 'low'\n",
    "        else:\n",
    "            score = 'high'\n",
    "        return score,\"reason\"\n",
    "\n",
    "result = my_metric.score(response='my response') # result\n",
    "result\n",
    "result.reason"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f60c70-fc32-41f8-aa7c-c8685d77398a",
   "metadata": {},
   "source": [
    "## Numeric Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "6a1c66fb-3c1c-4bc6-9996-0b5beb304b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@dataclass\n",
    "class NumericMetrics(Metric):\n",
    "    range: t.Tuple[float,float]\n",
    "    \n",
    "    def _get_response_model(self, with_reasoning: bool) -> t.Type[BaseModel]:\n",
    "        \"\"\"Get or create a response model based on reasoning parameter.\"\"\"\n",
    "        \n",
    "        if with_reasoning in self._response_models:\n",
    "            return self._response_models[with_reasoning]\n",
    "        \n",
    "        model_name = 'response_model'\n",
    "        fields = {\"result\": (float,...)}\n",
    "        \n",
    "        if with_reasoning:\n",
    "            fields[\"reason\"] = (str, ...)\n",
    "        \n",
    "        model = create_model(model_name, **fields)\n",
    "        self._response_models[with_reasoning] = model\n",
    "        return model \n",
    "\n",
    "    def _ensemble(self,results:t.List[MetricResult]) -> MetricResult:\n",
    "\n",
    "        if len(results)==1:\n",
    "            return results[0]\n",
    "    \n",
    "        candidates = [candidate.result for candidate in results]\n",
    "        result = sum(candidates)/len(candidates)\n",
    "        reason = results[0].reason\n",
    "    \n",
    "        return MetricResult(result=result,reason=reason)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "251cdea8-fc71-46bd-8a00-fb8e33e10350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The response lacks sufficient information or context to be considered helpful. It does not address any specific question or provide any useful insights.'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_metric = NumericMetrics(\n",
    "    name='helpfulness',\n",
    "    llm=LLM(),\n",
    "    prompt=\"Evaluate if given answer is helpful\\n\\n{response}\",\n",
    "    range=(0,10),\n",
    ")\n",
    "\n",
    "result = my_metric.score(response=\"this is my response\")\n",
    "result #gives \"low\"\n",
    "result.reason #gives reasoning from llm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b0994c80-c6db-4f3b-9ed9-1b32d61428c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_metric.batch_score(inputs=[{\"response\":\"this is my response\"}])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96520ae-294b-4868-8b2f-22a30ebd5f25",
   "metadata": {},
   "source": [
    "### decorator factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "265af384-ed35-4262-acfe-6847b22d3089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "numeric_metric = create_metric_decorator(NumericMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "009c1944-bda7-41b1-9235-dcda5acbed55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@numeric_metric(llm=LLM(),\n",
    "    prompt=\"Evaluate if given answer is helpful\\n\\n{response}\",\n",
    "    name='new_metric',range=(0,10))\n",
    "def my_metric(llm,prompt,**kwargs):\n",
    "\n",
    "        class response_model(BaseModel):\n",
    "             output: int\n",
    "             reason: str\n",
    "        \n",
    "        response = llm.generate(prompt.format(**kwargs),response_model=response_model)\n",
    "        total = response.output\n",
    "        if total < 1:\n",
    "            score = 0\n",
    "        else:\n",
    "            score = 10\n",
    "        return score,\"reason\"\n",
    "\n",
    "result = my_metric.score(response='my response') # result\n",
    "result # 10\n",
    "result.reason # the reason for the answer\n",
    "\n",
    "result1 = my_metric.score(response='my response 1') # result\n",
    "result2 = my_metric.score(response='my response 2') # result\n",
    "\n",
    "result1 + result2 # should be addable and behave like a float\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90794704-5e45-4dd5-8862-b4fb9694a5b5",
   "metadata": {},
   "source": [
    "### Ranking metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "9e2bb718-ba9a-4965-a952-462ac0159766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "typing.Literal[[0, 1, 2]]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.Literal[[i for i in range(3)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0b1bbf8f-c7fa-4004-9165-fa388f7ba15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@dataclass\n",
    "class RankingMetrics(Metric):\n",
    "    num_ranks: int\n",
    "    \n",
    "    def _get_response_model(self, with_reasoning: bool) -> t.Type[BaseModel]:\n",
    "        \"\"\"Get or create a response model based on reasoning parameter.\"\"\"\n",
    "        \n",
    "        if with_reasoning in self._response_models:\n",
    "            return self._response_models[with_reasoning]\n",
    "        \n",
    "        model_name = 'response_model'\n",
    "\n",
    "        # Custom validator to ensure 'result' is a permutation of 0 .. num_ranks-1\n",
    "        def validate_result(cls, v):\n",
    "            expected = set(range(self.num_ranks))\n",
    "            if set(v) != expected:\n",
    "                raise ValueError(\n",
    "                    f\"'result' must contain exactly the numbers {sorted(expected)} without repetition.\"\n",
    "                )\n",
    "            return v\n",
    "\n",
    "        # Define the fields dynamically\n",
    "        fields = {\"result\": (List[int], ...)}\n",
    "        if with_reasoning:\n",
    "            fields[\"reason\"] = (str, ...)\n",
    "        \n",
    "        # Create the dynamic model with the custom validator attached\n",
    "        model = create_model(\n",
    "            model_name,\n",
    "            **fields,\n",
    "            __validators__={\n",
    "                'result_validator': validator('result', allow_reuse=True)(validate_result)\n",
    "            }\n",
    "        )\n",
    "        self._response_models[with_reasoning] = model\n",
    "        return model \n",
    "\n",
    "    def _ensemble(self,results:t.List[MetricResult]) -> MetricResult:\n",
    "\n",
    "        if len(results)==1:\n",
    "            return results[0]\n",
    "\n",
    "        n_items = len(results)\n",
    "        borda_scores = [0] * n_items\n",
    "\n",
    "        for result in results:\n",
    "            for position_idx,item_idx in enumerate(result.result):\n",
    "                borda_scores[item_idx] += (n_items - (position_idx-1))\n",
    "\n",
    "        indexed_scores = [(score, i) for i, score in enumerate(borda_scores)]    \n",
    "        indexed_scores.sort(key=lambda x: (-x[0], x[1]))    \n",
    "        final_ranking = [pos for _, pos in indexed_scores]\n",
    "\n",
    "        if any(r.reason for r in results):\n",
    "            reason = \"Ensemble ranking based on multiple evaluations.\\n\" + '\\n'.join([r.reason for r in results if r.reason])\n",
    "        else:\n",
    "            reason = None\n",
    "        \n",
    "        \n",
    "        return MetricResult(result=result,reason=reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716881a1-0a93-46b3-b41b-aee0f987a1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ww/sk5dkfhn673234cmy5w7008r0000gn/T/ipykernel_95467/972172485.py:40: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  'result_validator': validator('result', allow_reuse=True)(validate_result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n",
      "Ensemble ranking based on multiple evaluations.\n",
      "The responses are ranked from the shortest to the longest and most detailed.\n",
      "The responses are ranked from shortest to longest and most detailed.\n",
      "Responses ranked from shortest to longest.\n"
     ]
    }
   ],
   "source": [
    "# User instantiates a ranking metric by providing a name, an LLM, a prompt template, and the number of rankings desired.\n",
    "my_ranking_metric = RankingMetrics(\n",
    "    name='response_ranking',\n",
    "    llm=LLM(),  # Your language model instance\n",
    "    prompt=\"Rank the following responses:\\n{candidates}\",\n",
    "    num_ranks=3,\n",
    ")\n",
    "\n",
    "# To score a single input (ranking candidate responses)\n",
    "result = my_ranking_metric.score(candidates=[\n",
    "    \"short answer.\",\n",
    "    \"a bit more detailed.\",\n",
    "    \"the longest and most detailed answer.\"\n",
    "],n=3)\n",
    "print(result)   # Might output something like: [1, 0, 2]\n",
    "print(result.reason)  # Provides the reasoning behind the ranking\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e198d7d-fbab-448e-aab1-f10f4234dff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b53bd5e-06c9-4430-9c06-f2225ddd7bd5",
   "metadata": {},
   "source": [
    "### decorator factory for ranking metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4c4e9170-67b9-4841-9df2-6afc490b89dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "ranking_metric = create_metric_decorator(RankingMetrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cbb1729b-8b25-48d8-a472-c03dd1e0d861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2]\n",
      "Ranked based on response clarity and detail.\n"
     ]
    }
   ],
   "source": [
    "@ranking_metric(\n",
    "    llm=LLM(),  # Your language model instance\n",
    "    prompt=\"Rank the following responses:\\n{candidates}\",\n",
    "    name='new_ranking_metric',\n",
    "    num_ranks=3\n",
    ")\n",
    "def my_ranking_metric(llm, prompt, **kwargs):\n",
    "    # Your custom logic that calls the LLM and returns a tuple of (ranking, reason)\n",
    "    # For example, process the prompt (formatted with candidates) and produce a ranking.\n",
    "    ranking = [1, 0, 2]  # Dummy ranking: second candidate is best, then first, then third.\n",
    "    reason = \"Ranked based on response clarity and detail.\"\n",
    "    return ranking, reason\n",
    "\n",
    "# Using the decorator-based ranking metric:\n",
    "result = my_ranking_metric.score(candidates=[\n",
    "    \"Response A: short answer.\",\n",
    "    \"Response B: a bit more detailed.\",\n",
    "    \"Response C: the longest and most detailed answer.\"\n",
    "])\n",
    "print(result)   # E.g., [1, 0, 2]\n",
    "print(result.reason)  # E.g., \"Ranked based on response clarity and detail.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e38ce5-aac9-489b-96c0-947011dbbdf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "random",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
