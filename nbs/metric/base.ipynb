{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ef8db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp metric.base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125fcb9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/Users/shahules/Myprojects/ragas_annotator/.envrc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb8f806",
   "metadata": {},
   "source": [
    "# BaseMetric\n",
    "> base class for all type of metrics in ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ccff58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/random/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "import asyncio\n",
    "from dataclasses import dataclass, field\n",
    "from pydantic import BaseModel\n",
    "import typing as t\n",
    "from ragas_annotator.metric import MetricResult\n",
    "from ragas_annotator.metric import LLM\n",
    "\n",
    "@dataclass\n",
    "class Metric(ABC):\n",
    "    \"\"\"Base class for all metrics in the LLM evaluation library.\"\"\"\n",
    "    name: str\n",
    "    prompt: str\n",
    "    llm: LLM\n",
    "    _response_models: t.Dict[bool, t.Type[BaseModel]] = field(\n",
    "        default_factory=dict, init=False, repr=False\n",
    "    )\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _get_response_model(self, with_reasoning: bool) -> t.Type[BaseModel]:\n",
    "        \"\"\"Get the appropriate response model.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _ensemble(self, results: t.List[MetricResult]) -> MetricResult:\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    def score(self, reasoning: bool = True, n: int = 1, **kwargs) -> t.Any:\n",
    "        responses = []\n",
    "        prompt_input = self.prompt.format(**kwargs)\n",
    "        for _ in range(n):\n",
    "            response = self.llm.generate(prompt_input, response_model = self._get_response_model(reasoning)) \n",
    "            response = MetricResult(**response.model_dump())\n",
    "            responses.append(response)\n",
    "        return self._ensemble(responses)\n",
    "\n",
    "\n",
    "    async def ascore(self, reasoning: bool = True, n: int = 1, **kwargs) -> MetricResult:\n",
    "        responses = []  # Added missing initialization\n",
    "        prompt_input = self.prompt.format(**kwargs)\n",
    "        for _ in range(n):\n",
    "            response = await self.llm.agenerate(prompt_input, response_model = self._get_response_model(reasoning))\n",
    "            response = MetricResult(**response.model_dump())  # Fixed missing parentheses\n",
    "            responses.append(response)\n",
    "        return self._ensemble(responses)\n",
    "        \n",
    "    def batch_score(self, inputs: t.List[t.Dict[str, t.Any]], reasoning: bool = True, n: int = 1) -> t.List[t.Any]:\n",
    "        return [self.score(reasoning, n, **input_dict) for input_dict in inputs]\n",
    "    \n",
    "    async def abatch_score(self, inputs: t.List[t.Dict[str, t.Any]], reasoning: bool = True, n: int = 1) -> t.List[MetricResult]:\n",
    "        async_tasks = []\n",
    "        for input_dict in inputs:\n",
    "            # Add reasoning and n to the input parameters\n",
    "            async_tasks.append(self.ascore(reasoning=reasoning, n=n, **input_dict))\n",
    "            \n",
    "        # Run all tasks concurrently and return results\n",
    "        return await asyncio.gather(*async_tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4b7458",
   "metadata": {},
   "source": [
    "### Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf208fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "@dataclass\n",
    "class CustomMetric(Metric):\n",
    "    values: t.List[str] = field(default_factory=lambda: [\"pass\", \"fail\"])\n",
    "    \n",
    "    def _get_response_model(self, with_reasoning: bool) -> t.Type[BaseModel]:\n",
    "        \"\"\"Get or create a response model based on reasoning parameter.\"\"\"\n",
    "        \n",
    "        class mymodel(BaseModel):\n",
    "            result: int\n",
    "            reason: t.Optional[str] = None\n",
    "            \n",
    "        return mymodel \n",
    "\n",
    "    def _ensemble(self,results:t.List[MetricResult]) -> MetricResult:\n",
    "        \n",
    "        return results[0]  # Placeholder for ensemble logic\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba99094",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_metric = CustomMetric(name=\"example\", prompt=\"What is the result of {input}?\", llm=LLM())\n",
    "my_metric.score(input=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1327f250",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
