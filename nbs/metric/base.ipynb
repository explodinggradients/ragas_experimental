{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ef8db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp metric.base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb8f806",
   "metadata": {},
   "source": [
    "# BaseMetric\n",
    "> base class for all type of metrics in ragas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ccff58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/random/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "import asyncio\n",
    "from dataclasses import dataclass, field\n",
    "from pydantic import BaseModel\n",
    "import typing as t\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ragas_annotator.prompt.base import Prompt\n",
    "from ragas_annotator.embedding.base import BaseEmbedding\n",
    "from ragas_annotator.metric import MetricResult\n",
    "from ragas_annotator.llm import RagasLLM\n",
    "from ragas_annotator.project.core import Project\n",
    "from ragas_annotator.model.notion_model import NotionModel\n",
    "from ragas_annotator.prompt.dynamic_few_shot import DynamicFewShotPrompt\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Metric(ABC):\n",
    "    \"\"\"Base class for all metrics in the LLM evaluation library.\"\"\"\n",
    "    name: str\n",
    "    prompt: str | Prompt\n",
    "    llm: RagasLLM\n",
    "    _response_models: t.Dict[bool, t.Type[BaseModel]] = field(\n",
    "        default_factory=dict, init=False, repr=False\n",
    "    )\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if isinstance(self.prompt,str):\n",
    "            self.prompt = Prompt(self.prompt)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _get_response_model(self, with_reasoning: bool) -> t.Type[BaseModel]:\n",
    "        \"\"\"Get the appropriate response model.\"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _ensemble(self, results: t.List[MetricResult]) -> MetricResult:\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    def score(self, reasoning: bool = True, n: int = 1, **kwargs) -> t.Any:\n",
    "        responses = []\n",
    "        traces = {}\n",
    "        traces[\"input\"] = kwargs\n",
    "        prompt_input = self.prompt.format(**kwargs)\n",
    "        for _ in range(n):\n",
    "            response = self.llm.generate(prompt_input, response_model = self._get_response_model(reasoning)) \n",
    "            traces['output'] = response.model_dump()\n",
    "            response = MetricResult(**response.model_dump())\n",
    "            responses.append(response)\n",
    "        results = self._ensemble(responses)\n",
    "        results.traces = traces\n",
    "        return results\n",
    "\n",
    "\n",
    "    async def ascore(self, reasoning: bool = True, n: int = 1, **kwargs) -> MetricResult:\n",
    "        responses = []  # Added missing initialization\n",
    "        traces = {}\n",
    "        traces[\"input\"] = kwargs\n",
    "        prompt_input = self.prompt.format(**kwargs)\n",
    "        for _ in range(n):\n",
    "            response = await self.llm.agenerate(prompt_input, response_model = self._get_response_model(reasoning))\n",
    "            traces['output'] = response.model_dump()\n",
    "            response = MetricResult(**response.model_dump())  # Fixed missing parentheses\n",
    "            responses.append(response)\n",
    "        results = self._ensemble(responses)\n",
    "        results.traces = traces\n",
    "        return results\n",
    "        \n",
    "    def batch_score(self, inputs: t.List[t.Dict[str, t.Any]], reasoning: bool = True, n: int = 1) -> t.List[t.Any]:\n",
    "        return [self.score(reasoning, n, **input_dict) for input_dict in inputs]\n",
    "    \n",
    "    async def abatch_score(self, inputs: t.List[t.Dict[str, t.Any]], reasoning: bool = True, n: int = 1) -> t.List[MetricResult]:\n",
    "        async_tasks = []\n",
    "        for input_dict in inputs:\n",
    "            # Add reasoning and n to the input parameters\n",
    "            async_tasks.append(self.ascore(reasoning=reasoning, n=n, **input_dict))\n",
    "            \n",
    "        # Run all tasks concurrently and return results\n",
    "        return await asyncio.gather(*async_tasks)\n",
    "    \n",
    "    def train(self,project:Project, experiment_names: t.List[str], model:NotionModel, embedding_model: BaseEmbedding,method: t.Dict[str, t.Any]):\n",
    "        \n",
    "        assert isinstance(self.prompt, Prompt)\n",
    "        self.prompt = DynamicFewShotPrompt.from_prompt(self.prompt,embedding_model)\n",
    "        datasets = []\n",
    "        for experiment_name in experiment_names:\n",
    "            experiment_data = project.get_experiment(experiment_name,model)\n",
    "            experiment_data.load()\n",
    "            datasets.append(experiment_data)\n",
    "        \n",
    "        total_items = sum([len(dataset) for dataset in datasets])\n",
    "        with tqdm(total=total_items, desc=\"Processing examples\") as pbar:\n",
    "            for dataset in datasets:\n",
    "                for row in dataset:\n",
    "                    if hasattr(row, f'{self.name}_traces'):\n",
    "                        traces = json.loads(getattr(row, f'{self.name}_traces'))\n",
    "                        if traces:\n",
    "                            self.prompt.add_example(traces['input'],traces['output'])\n",
    "                    pbar.update(1)\n",
    "        \n",
    "                \n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4b7458",
   "metadata": {},
   "source": [
    "### Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf208fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "from ragas_annotator.llm import ragas_llm\n",
    "from openai import OpenAI\n",
    "\n",
    "llm = ragas_llm(provider=\"openai\",model=\"gpt-4o\",client=OpenAI())\n",
    "\n",
    "@dataclass\n",
    "class CustomMetric(Metric):\n",
    "    values: t.List[str] = field(default_factory=lambda: [\"pass\", \"fail\"])\n",
    "    \n",
    "    def _get_response_model(self, with_reasoning: bool) -> t.Type[BaseModel]:\n",
    "        \"\"\"Get or create a response model based on reasoning parameter.\"\"\"\n",
    "        \n",
    "        class mymodel(BaseModel):\n",
    "            result: int\n",
    "            reason: t.Optional[str] = None\n",
    "            \n",
    "        return mymodel \n",
    "\n",
    "    def _ensemble(self,results:t.List[MetricResult]) -> MetricResult:\n",
    "        \n",
    "        return results[0]  # Placeholder for ensemble logic\n",
    "\n",
    "my_metric = CustomMetric(name=\"example\", prompt=\"What is the result of {input}?\", llm=llm)\n",
    "my_metric.score(input=\"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
