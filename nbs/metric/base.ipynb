{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c48aac0f-c63c-4bfb-95b0-a1239f41ccb3",
   "metadata": {},
   "source": [
    "# Base\n",
    "> baseclass for metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d903a59c-3ed8-4b7d-bb9d-39180df72950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp metric.base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d14fc66-b8af-4a75-b761-20b8e9ce19f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/Users/shahules/Myprojects/ragas_annotator/.envrc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b05c525-e153-49ab-b768-5069f624f215",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import typing as t\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "from abc import ABC, abstractmethod\n",
    "import asyncio\n",
    "from dataclasses import dataclass\n",
    "from pydantic import BaseModel\n",
    "import openai\n",
    "import instructor\n",
    "from dataclasses import dataclass, field\n",
    "from pydantic import BaseModel, create_model\n",
    "import typing as t\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5140197-08a5-46de-b9bd-818e9be8c951",
   "metadata": {},
   "source": [
    "### MetricResult\n",
    "> Class to hold the result metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56dbaeb3-e105-4daf-a5b2-a91cb1fb976f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as t\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "\n",
    "class MetricResult:\n",
    "    \"\"\"Class to hold the result of a metric evaluation.\n",
    "    \n",
    "    This class behaves like its underlying result value but still provides access\n",
    "    to additional metadata like reasoning.\n",
    "    \n",
    "    Works with:\n",
    "    - DiscreteMetrics (string results)\n",
    "    - NumericMetrics (float/int results)\n",
    "    - RankingMetrics (list results)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, result: Any, reason: t.Optional[str] = None):\n",
    "        self._result = result\n",
    "        self.reason = reason\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return repr(self._result)\n",
    "    \n",
    "    # Access to underlying result\n",
    "    @property\n",
    "    def result(self):\n",
    "        \"\"\"Get the raw result value.\"\"\"\n",
    "        return self._result\n",
    "    \n",
    "    \n",
    "    # String conversion - works for all types\n",
    "    def __str__(self):\n",
    "        return str(self._result)\n",
    "    \n",
    "    # Container-like behaviors for list results (RankingMetric)\n",
    "    def __getitem__(self, key):\n",
    "        if not hasattr(self._result, \"__getitem__\"):\n",
    "            raise TypeError(f\"{type(self._result).__name__} object is not subscriptable\")\n",
    "        return self._result[key]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        if not hasattr(self._result, \"__iter__\"):\n",
    "            raise TypeError(f\"{type(self._result).__name__} object is not iterable\")\n",
    "        return iter(self._result)\n",
    "    \n",
    "    def __len__(self):\n",
    "        if not hasattr(self._result, \"__len__\"):\n",
    "            raise TypeError(f\"{type(self._result).__name__} has no len()\")\n",
    "        return len(self._result)\n",
    "    \n",
    "    # Numeric operations for numeric results (NumericMetric)\n",
    "    def __float__(self):\n",
    "        if isinstance(self._result, (int, float)):\n",
    "            return float(self._result)\n",
    "        raise TypeError(f\"Cannot convert {type(self._result).__name__} to float\")\n",
    "    \n",
    "    def __int__(self):\n",
    "        if isinstance(self._result, (int, float)):\n",
    "            return int(self._result)\n",
    "        raise TypeError(f\"Cannot convert {type(self._result).__name__} to int\")\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        if not isinstance(self._result, (int, float)):\n",
    "            raise TypeError(f\"Cannot add {type(self._result).__name__} objects\")\n",
    "        if isinstance(other, MetricResult):\n",
    "            return self._result + other._result\n",
    "        return self._result + other\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        if not isinstance(self._result, (int, float)):\n",
    "            raise TypeError(f\"Cannot add {type(self._result).__name__} objects\")\n",
    "        return other + self._result\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        if not isinstance(self._result, (int, float)):\n",
    "            raise TypeError(f\"Cannot subtract {type(self._result).__name__} objects\")\n",
    "        if isinstance(other, MetricResult):\n",
    "            return self._result - other._result\n",
    "        return self._result - other\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        if not isinstance(self._result, (int, float)):\n",
    "            raise TypeError(f\"Cannot subtract {type(self._result).__name__} objects\")\n",
    "        return other - self._result\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        if not isinstance(self._result, (int, float)):\n",
    "            raise TypeError(f\"Cannot multiply {type(self._result).__name__} objects\")\n",
    "        if isinstance(other, MetricResult):\n",
    "            return self._result * other._result\n",
    "        return self._result * other\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        if not isinstance(self._result, (int, float)):\n",
    "            raise TypeError(f\"Cannot multiply {type(self._result).__name__} objects\")\n",
    "        return other * self._result\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        if not isinstance(self._result, (int, float)):\n",
    "            raise TypeError(f\"Cannot divide {type(self._result).__name__} objects\")\n",
    "        if isinstance(other, MetricResult):\n",
    "            return self._result / other._result\n",
    "        return self._result / other\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "        if not isinstance(self._result, (int, float)):\n",
    "            raise TypeError(f\"Cannot divide {type(self._result).__name__} objects\")\n",
    "        return other / self._result\n",
    "    \n",
    "    # Comparison operations - work for all types with same-type comparisons\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, MetricResult):\n",
    "            return self._result == other._result\n",
    "        return self._result == other\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        if isinstance(other, MetricResult):\n",
    "            return self._result < other._result\n",
    "        return self._result < other\n",
    "    \n",
    "    def __le__(self, other):\n",
    "        if isinstance(other, MetricResult):\n",
    "            return self._result <= other._result\n",
    "        return self._result <= other\n",
    "    \n",
    "    def __gt__(self, other):\n",
    "        if isinstance(other, MetricResult):\n",
    "            return self._result > other._result\n",
    "        return self._result > other\n",
    "    \n",
    "    def __ge__(self, other):\n",
    "        if isinstance(other, MetricResult):\n",
    "            return self._result >= other._result\n",
    "        return self._result >= other\n",
    "    \n",
    "    # Method forwarding for type-specific behaviors\n",
    "    def __getattr__(self, name):\n",
    "        \"\"\"Forward attribute access to the result object if it has that attribute.\n",
    "        \n",
    "        This allows calling string methods on discrete results, \n",
    "        numeric methods on numeric results, and list methods on ranking results.\n",
    "        \"\"\"\n",
    "        if hasattr(self._result, name):\n",
    "            attr = getattr(self._result, name)\n",
    "            if callable(attr):\n",
    "                # If it's a method, wrap it to return MetricResult when appropriate\n",
    "                def wrapper(*args, **kwargs):\n",
    "                    result = attr(*args, **kwargs)\n",
    "                    # If the result is of the same type as self._result, wrap it\n",
    "                    if isinstance(result, type(self._result)):\n",
    "                        return MetricResult(result=result, reason=self.reason)\n",
    "                    return result\n",
    "                return wrapper\n",
    "            return attr\n",
    "        raise AttributeError(f\"{type(self).__name__} has no attribute '{name}'\")\n",
    "    \n",
    "    # JSON/dict serialization\n",
    "    def to_dict(self):\n",
    "        \"\"\"Convert the result to a dictionary.\"\"\"\n",
    "        return {\n",
    "            \"result\": self._result,\n",
    "            \"reason\": self.reason\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89555bee-23a3-4129-86a5-0a2ffeed00c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'low'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = MetricResult(result='low',reason=\"my reason\")\n",
    "result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f21b7a50-3142-40e9-9612-0603f5b8654d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my reason'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7f588f-2a13-4c10-9775-00101a03e0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LLM:\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.aclient = instructor.from_openai(openai.AsyncOpenAI())\n",
    "        self.client = instructor.from_openai(openai.OpenAI())\n",
    "\n",
    "    \n",
    "    def generate(self,prompt,response_model):\n",
    "        return self.client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        response_model=response_model,\n",
    "    )\n",
    "\n",
    "    async def agenerate(self,prompt,response_model):\n",
    "        return await self.aclient.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        response_model=response_model,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bec7946-c61c-4631-9880-fff575974e39",
   "metadata": {},
   "source": [
    "### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6fecbe-4fdd-464e-961e-48a528bc3278",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5e19478-d947-405b-a229-4a1e7daa2fd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataclass' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#| export\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mMetric\u001b[39;00m(ABC):\n\u001b[1;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Base class for all metrics in the LLM evaluation library.\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     name:\u001b[38;5;28mstr\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataclass' is not defined"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "\n",
    "@dataclass\n",
    "class Metric(ABC):\n",
    "    \"\"\"Base class for all metrics in the LLM evaluation library.\"\"\"\n",
    "    name:str\n",
    "    prompt:str\n",
    "    llm:LLM\n",
    "    _response_models: Dict[bool, Type[BaseModel]] = field(\n",
    "        default_factory=dict, init=False, repr=False\n",
    "    )\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _get_response_model(self, with_reasoning: bool) -> Type[BaseModel]:\n",
    "        \"\"\"Get the appropriate response model.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def score(self, reasoning=True, n=1, **kwargs) -> Any:\n",
    "        \n",
    "        prompt_input = self.prompt.format(**kwargs)\n",
    "        response = self.llm.generate(prompt_input, response_model = self._get_response_model(reasoning))\n",
    "        return MetricResult(**response.model_dump())\n",
    "\n",
    "    async def ascore(self, reasoning=True, n=1, **kwargs):\n",
    "        prompt_input = self.prompt.format(**kwargs)\n",
    "        response = await self.llm.agenerate(prompt_input, response_model = self._get_response_model(reasoning))\n",
    "        return MetricResult(**response.model_dump())\n",
    "        \n",
    "    def batch_score(self, inputs: List[Dict[str, Any]], reasoning:bool=True, n:int=1) -> List[Any]:\n",
    "       \n",
    "        return [self.score(**input_dict) for input_dict in inputs]\n",
    "    \n",
    "    async def abatch_score(self, inputs: List[Dict[str, Any]], reasoning: bool = True, n: int = 1) -> List[MetricResult]:\n",
    "        \n",
    "        async_tasks = []\n",
    "        for input_dict in inputs:\n",
    "            # Add reasoning and n to the input parameters\n",
    "            async_tasks.append(self.ascore(reasoning=reasoning, n=n, **input_dict))\n",
    "            \n",
    "        # Run all tasks concurrently and return results\n",
    "        return await asyncio.gather(*async_tasks)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af79bb4-4c3e-4004-b7f3-ec36e50b4ca5",
   "metadata": {},
   "source": [
    "### DiscreteMetric \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f09683e0-5ec3-4e60-a8c4-1657e2fe60b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@dataclass\n",
    "class DiscreteMetrics(Metric):\n",
    "    values: t.List[str] = field(default_factory=lambda: [\"pass\", \"fail\"])\n",
    "    _response_models: t.Dict[bool, t.Type[BaseModel]] = field(default_factory=dict, init=False, repr=False)\n",
    "    \n",
    "    def _get_response_model(self, with_reasoning: bool) -> t.Type[BaseModel]:\n",
    "        \"\"\"Get or create a response model based on reasoning parameter.\"\"\"\n",
    "        \n",
    "        if with_reasoning in self._response_models:\n",
    "            return self._response_models[with_reasoning]\n",
    "        \n",
    "        model_name = 'response_model'\n",
    "        fields = {\"score\": (t.Literal[tuple(self.values)], ...)}\n",
    "        \n",
    "        if with_reasoning:\n",
    "            fields[\"reason\"] = (str, ...)\n",
    "        \n",
    "        model = create_model(model_name, **fields)\n",
    "        self._response_models[with_reasoning] = model\n",
    "        return model \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33ef89f-4ccc-4307-9944-4f372ce77830",
   "metadata": {},
   "source": [
    "### decorator factory for discrete_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99d5afd6-72bd-42d7-bff0-effce9cf8cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def discrete_metric(llm, prompt, values:t.List[str],name:t.Optional[str]=None):\n",
    "\n",
    "    def decorator(metric):\n",
    "        metric_name = name or metric.__name__ \n",
    "        is_async = inspect.iscoroutinefunction(metric)\n",
    "\n",
    "        @dataclass\n",
    "        class CustomDiscreteMetric(DiscreteMetrics):\n",
    "\n",
    "            def score(self,reasoning:bool=True, n:int=1,**kwargs):\n",
    "\n",
    "                if is_async:\n",
    "                    # For async functions, we need to run them in an event loop\n",
    "                    import asyncio\n",
    "                    \n",
    "                    # Get or create an event loop\n",
    "                    try:\n",
    "                        loop = asyncio.get_event_loop()\n",
    "                    except RuntimeError:\n",
    "                        loop = asyncio.new_event_loop()\n",
    "                        asyncio.set_event_loop(loop)\n",
    "                    \n",
    "                    # Run the async function and get the result\n",
    "                    result = loop.run_until_complete(metric(self.llm,self.prompt,**kwargs))\n",
    "                else:\n",
    "                    # For sync functions, just call directly\n",
    "                    result = metric(self.llm,self.prompt,**kwargs)\n",
    "                    \n",
    "                if isinstance(result, tuple) and len(result) == 2:\n",
    "                    score, reason = result\n",
    "                else:\n",
    "                    score = result\n",
    "                    reason = None\n",
    "                    \n",
    "                return MetricResult(score=score, reason=reason if reasoning else None)\n",
    "            \n",
    "            async def ascore(self, reasoning=True, n=1, **kwargs):\n",
    "                if is_async:\n",
    "                    # For async functions, await them directly\n",
    "                    result = await metric(self.llm,self.prompt,**kwargs)\n",
    "                else:\n",
    "                    # For sync functions, run them normally\n",
    "                    result = metric(self.llm,self.prompt,**kwargs)\n",
    "\n",
    "                if isinstance(result, tuple) and len(result) == 2:\n",
    "                    score, reason = result\n",
    "                else:\n",
    "                    score = result\n",
    "                    reason = None\n",
    "                    \n",
    "        metric_instance = CustomDiscreteMetric(\n",
    "            name=metric_name,\n",
    "            prompt=prompt,\n",
    "            llm=llm,\n",
    "            values=values,\n",
    "            \n",
    "        )\n",
    "        metric_instance.__name__ = name\n",
    "        metric_instance.__doc__ = metric.__doc__\n",
    "        \n",
    "        return metric_instance\n",
    "    \n",
    "    return decorator\n",
    "            \n",
    "    \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f49ad3-4476-4bcc-ac34-a87fb7a8652a",
   "metadata": {},
   "source": [
    "### Usage pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aeae8fe5-e81a-44ac-9ad7-a240655a0f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_metric = DiscreteMetrics(\n",
    "    name='helpfulness',\n",
    "    llm=LLM(),\n",
    "    prompt=\"Evaluate if given answer is helpful\\n\\n{response}\",\n",
    "    values=[\"low\",\"med\",\"high\"],\n",
    ")\n",
    "\n",
    "result = my_metric.score(response=\"this is my response\")\n",
    "result #gives \"low\"\n",
    "result.reason #gives reasoning from llm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5e499d8-8258-46ce-b719-0389d3cfd8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetricResult(score=low, reason=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## score without reasoning to save reasoning tokens cost\n",
    "result = my_metric.score(response=\"this is my response\",reasoning=False)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a9a5d6c6-4cfc-4f45-8b19-996315a95370",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reason'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@discrete_metric(llm=LLM(),\n",
    "    prompt=\"Evaluate if given answer is helpful\\n\\n{response}\",\n",
    "    name='new_metric',values=[\"low\",\"med\",\"high\"])\n",
    "def my_metric(llm,prompt,**kwargs):\n",
    "\n",
    "        class response_model(BaseModel):\n",
    "             output: t.List[bool]\n",
    "             reason: str\n",
    "        \n",
    "        response = llm.generate(prompt.format(**kwargs),response_model=response_model)\n",
    "        total = sum(response.output)\n",
    "        if total < 1:\n",
    "            score = 'low'\n",
    "        else:\n",
    "            score = 'high'\n",
    "        return score,\"reason\"\n",
    "\n",
    "result = my_metric.score(response='my response') # result\n",
    "result.score\n",
    "result.reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b4809ca-d921-4084-bb9e-fe3a72f438a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MetricResult(score=high, reason=reason)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f60c70-fc32-41f8-aa7c-c8685d77398a",
   "metadata": {},
   "source": [
    "## Numeric Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a1c66fb-3c1c-4bc6-9996-0b5beb304b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@dataclass\n",
    "class NumericMetrics(Metric):\n",
    "    range: t.Tuple[float,float]\n",
    "    _response_models: t.Dict[bool, t.Type[BaseModel]] = field(default_factory=dict, init=False, repr=False)\n",
    "    \n",
    "    def _get_response_model(self, with_reasoning: bool) -> t.Type[BaseModel]:\n",
    "        \"\"\"Get or create a response model based on reasoning parameter.\"\"\"\n",
    "        \n",
    "        if with_reasoning in self._response_models:\n",
    "            return self._response_models[with_reasoning]\n",
    "        \n",
    "        model_name = 'response_model'\n",
    "        fields = {\"score\": (float,...)}\n",
    "        \n",
    "        if with_reasoning:\n",
    "            fields[\"reason\"] = (str, ...)\n",
    "        \n",
    "        model = create_model(model_name, **fields)\n",
    "        self._response_models[with_reasoning] = model\n",
    "        return model \n",
    "    \n",
    "    def score(self, reasoning=True, n=1, **kwargs) -> Any:\n",
    "        \n",
    "        prompt_input = self.prompt.format(**kwargs)\n",
    "        response = self.llm.generate(prompt_input, response_model = self._get_response_model(reasoning))\n",
    "        return MetricResult(**response.model_dump())\n",
    "\n",
    "    async def ascore(self, reasoning=True, n=1, **kwargs):\n",
    "        prompt_input = self.prompt.format(**kwargs)\n",
    "        response = await self.llm.agenerate(prompt_input, response_model = self._get_response_model(reasoning))\n",
    "        return MetricResult(**response.model_dump())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "251cdea8-fc71-46bd-8a00-fb8e33e10350",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_metric = NumericMetrics(\n",
    "    name='helpfulness',\n",
    "    llm=LLM(),\n",
    "    prompt=\"Evaluate if given answer is helpful\\n\\n{response}\",\n",
    "    range=(0,10),\n",
    ")\n",
    "\n",
    "result = my_metric.score(response=\"this is my response\")\n",
    "result #gives \"low\"\n",
    "result.reason #gives reasoning from llm\n",
    "\n",
    "result = my_metric.batch_score(inputs=[{\"response\":\"this is my response\"}])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96520ae-294b-4868-8b2f-22a30ebd5f25",
   "metadata": {},
   "source": [
    "### decorator factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "265af384-ed35-4262-acfe-6847b22d3089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "def numeric_metric(llm, prompt, range: t.Tuple[float,float],name:t.Optional[str]=None):\n",
    "\n",
    "    def decorator(metric):\n",
    "        metric_name = name or metric.__name__ \n",
    "        is_async = inspect.iscoroutinefunction(metric)\n",
    "\n",
    "        @dataclass\n",
    "        class CustomNumericMetric(NumericMetrics):\n",
    "\n",
    "            def score(self,reasoning:bool=True, n:int=1,**kwargs):\n",
    "\n",
    "                if is_async:\n",
    "                    # For async functions, we need to run them in an event loop\n",
    "                    import asyncio\n",
    "                    \n",
    "                    # Get or create an event loop\n",
    "                    try:\n",
    "                        loop = asyncio.get_event_loop()\n",
    "                    except RuntimeError:\n",
    "                        loop = asyncio.new_event_loop()\n",
    "                        asyncio.set_event_loop(loop)\n",
    "                    \n",
    "                    # Run the async function and get the result\n",
    "                    result = loop.run_until_complete(metric(self.llm,self.prompt,**kwargs))\n",
    "                else:\n",
    "                    # For sync functions, just call directly\n",
    "                    result = metric(self.llm,self.prompt,**kwargs)\n",
    "                    \n",
    "                if isinstance(result, tuple) and len(result) == 2:\n",
    "                    score, reason = result\n",
    "                else:\n",
    "                    score = result\n",
    "                    reason = None\n",
    "                    \n",
    "                return MetricResult(score=score, reason=reason if reasoning else None)\n",
    "            \n",
    "            async def ascore(self, reasoning=True, n=1, **kwargs):\n",
    "                if is_async:\n",
    "                    # For async functions, await them directly\n",
    "                    result = await metric(self.llm,self.prompt,**kwargs)\n",
    "                else:\n",
    "                    # For sync functions, run them normally\n",
    "                    result = metric(self.llm,self.prompt,**kwargs)\n",
    "\n",
    "                if isinstance(result, tuple) and len(result) == 2:\n",
    "                    score, reason = result\n",
    "                else:\n",
    "                    score = result\n",
    "                    reason = None\n",
    "                    \n",
    "        metric_instance = CustomNumericMetric(\n",
    "            name=metric_name,\n",
    "            prompt=prompt,\n",
    "            llm=llm,\n",
    "            range=range,\n",
    "            \n",
    "        )\n",
    "        metric_instance.__name__ = name\n",
    "        metric_instance.__doc__ = metric.__doc__\n",
    "        \n",
    "        return metric_instance\n",
    "    \n",
    "    return decorator\n",
    "            \n",
    "    \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "009c1944-bda7-41b1-9235-dcda5acbed55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reason'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@numeric_metric(llm=LLM(),\n",
    "    prompt=\"Evaluate if given answer is helpful\\n\\n{response}\",\n",
    "    name='new_metric',range=(0,10))\n",
    "def my_metric(llm,prompt,**kwargs):\n",
    "\n",
    "        class response_model(BaseModel):\n",
    "             output: int\n",
    "             reason: str\n",
    "        \n",
    "        response = llm.generate(prompt.format(**kwargs),response_model=response_model)\n",
    "        total = response.output\n",
    "        if total < 1:\n",
    "            score = 0\n",
    "        else:\n",
    "            score = 10\n",
    "        return score,\"reason\"\n",
    "\n",
    "result = my_metric.score(response='my response') # result\n",
    "result # 10\n",
    "result.reason # the reason for the answer\n",
    "\n",
    "result1 = my_metric.score(response='my response 1') # result\n",
    "result2 = my_metric.score(response='my response 2') # result\n",
    "\n",
    "result1 + result2 # should be addable and behave like a float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff32e972-0900-4ff1-94db-1378302f8d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90794704-5e45-4dd5-8862-b4fb9694a5b5",
   "metadata": {},
   "source": [
    "### Ranking metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0b1bbf8f-c7fa-4004-9165-fa388f7ba15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "@dataclass\n",
    "class RankingMetrics(Metric):\n",
    "    num_ranks: int\n",
    "    _response_models: t.Dict[bool, t.Type[BaseModel]] = field(default_factory=dict, init=False, repr=False)\n",
    "    \n",
    "    def _get_response_model(self, with_reasoning: bool) -> t.Type[BaseModel]:\n",
    "        \"\"\"Get or create a response model based on reasoning parameter.\"\"\"\n",
    "        \n",
    "        if with_reasoning in self._response_models:\n",
    "            return self._response_models[with_reasoning]\n",
    "        \n",
    "        model_name = 'response_model'\n",
    "        fields = {\"score\": (t.List[int],...)}\n",
    "        \n",
    "        if with_reasoning:\n",
    "            fields[\"reason\"] = (str, ...)\n",
    "        \n",
    "        model = create_model(model_name, **fields)\n",
    "        self._response_models[with_reasoning] = model\n",
    "        return model \n",
    "    \n",
    "    def score(self, reasoning=True, n=1, **kwargs) -> Any:\n",
    "        \n",
    "        prompt_input = self.prompt.format(**kwargs)\n",
    "        response = self.llm.generate(prompt_input, response_model = self._get_response_model(reasoning))\n",
    "        return MetricResult(**response.model_dump())\n",
    "\n",
    "    async def ascore(self, reasoning=True, n=1, **kwargs):\n",
    "        prompt_input = self.prompt.format(**kwargs)\n",
    "        response = await self.llm.agenerate(prompt_input, response_model = self._get_response_model(reasoning))\n",
    "        return MetricResult(**response.model_dump())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "716881a1-0a93-46b3-b41b-aee0f987a1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "The responses are ranked based on their length and detail, with the longest and most detailed answer receiving the highest rank.\n"
     ]
    }
   ],
   "source": [
    "# User instantiates a ranking metric by providing a name, an LLM, a prompt template, and the number of rankings desired.\n",
    "my_ranking_metric = RankingMetrics(\n",
    "    name='response_ranking',\n",
    "    llm=LLM(),  # Your language model instance\n",
    "    prompt=\"Rank the following responses:\\n{candidates}\",\n",
    "    num_ranks=3\n",
    ")\n",
    "\n",
    "# To score a single input (ranking candidate responses)\n",
    "result = my_ranking_metric.score(candidates=[\n",
    "    \"short answer.\",\n",
    "    \"a bit more detailed.\",\n",
    "    \"the longest and most detailed answer.\"\n",
    "])\n",
    "print(result.score)   # Might output something like: [1, 0, 2]\n",
    "print(result.reason)  # Provides the reasoning behind the ranking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b53bd5e-06c9-4430-9c06-f2225ddd7bd5",
   "metadata": {},
   "source": [
    "### decorator factory for ranking metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4c4e9170-67b9-4841-9df2-6afc490b89dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def ranking_metric(llm, prompt, num_ranks: int, name: t.Optional[str] = None):\n",
    "    def decorator(metric):\n",
    "        metric_name = name or metric.__name__\n",
    "        is_async = inspect.iscoroutinefunction(metric)\n",
    "\n",
    "        @dataclass\n",
    "        class CustomRankingMetric(RankingMetrics):\n",
    "            # Inherits: name, prompt, llm, num_ranks from RankingMetrics.\n",
    "            # No extra fields are needed.\n",
    "            def score(self, reasoning: bool = True, n: int = 1, **kwargs):\n",
    "                if is_async:\n",
    "                    # For async functions, run in an event loop.\n",
    "                    import asyncio\n",
    "                    try:\n",
    "                        loop = asyncio.get_event_loop()\n",
    "                    except RuntimeError:\n",
    "                        loop = asyncio.new_event_loop()\n",
    "                        asyncio.set_event_loop(loop)\n",
    "                    result = loop.run_until_complete(metric(self.llm, self.prompt, **kwargs))\n",
    "                else:\n",
    "                    result = metric(self.llm, self.prompt, **kwargs)\n",
    "\n",
    "                if isinstance(result, tuple) and len(result) == 2:\n",
    "                    ranking, reason = result\n",
    "                else:\n",
    "                    ranking = result\n",
    "                    reason = None\n",
    "\n",
    "                return MetricResult(score=ranking, reason=reason if reasoning else None)\n",
    "\n",
    "            async def ascore(self, reasoning: bool = True, n: int = 1, **kwargs):\n",
    "                if is_async:\n",
    "                    result = await metric(self.llm, self.prompt, **kwargs)\n",
    "                else:\n",
    "                    result = metric(self.llm, self.prompt, **kwargs)\n",
    "\n",
    "                if isinstance(result, tuple) and len(result) == 2:\n",
    "                    ranking, reason = result\n",
    "                else:\n",
    "                    ranking = result\n",
    "                    reason = None\n",
    "\n",
    "                return MetricResult(score=ranking, reason=reason if reasoning else None)\n",
    "\n",
    "        metric_instance = CustomRankingMetric(\n",
    "            name=metric_name,\n",
    "            prompt=prompt,\n",
    "            llm=llm,\n",
    "            num_ranks=num_ranks\n",
    "        )\n",
    "        metric_instance.__name__ = metric_name\n",
    "        metric_instance.__doc__ = metric.__doc__\n",
    "\n",
    "        return metric_instance\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cbb1729b-8b25-48d8-a472-c03dd1e0d861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 2]\n",
      "Ranked based on response clarity and detail.\n"
     ]
    }
   ],
   "source": [
    "@ranking_metric(\n",
    "    llm=LLM(),  # Your language model instance\n",
    "    prompt=\"Rank the following responses:\\n{candidates}\",\n",
    "    name='new_ranking_metric',\n",
    "    num_ranks=3\n",
    ")\n",
    "def my_ranking_metric(llm, prompt, **kwargs):\n",
    "    # Your custom logic that calls the LLM and returns a tuple of (ranking, reason)\n",
    "    # For example, process the prompt (formatted with candidates) and produce a ranking.\n",
    "    ranking = [1, 0, 2]  # Dummy ranking: second candidate is best, then first, then third.\n",
    "    reason = \"Ranked based on response clarity and detail.\"\n",
    "    return ranking, reason\n",
    "\n",
    "# Using the decorator-based ranking metric:\n",
    "result = my_ranking_metric.score(candidates=[\n",
    "    \"Response A: short answer.\",\n",
    "    \"Response B: a bit more detailed.\",\n",
    "    \"Response C: the longest and most detailed answer.\"\n",
    "])\n",
    "print(result.score)   # E.g., [1, 0, 2]\n",
    "print(result.reason)  # E.g., \"Ranked based on response clarity and detail.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e6646a-c65a-4317-994c-43aae31d65e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (random)",
   "language": "python",
   "name": "random"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
